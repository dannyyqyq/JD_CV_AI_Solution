{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5439e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f11748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPDFLoader\n",
      "Error: File path data/MLE_CV_2025.pdf is not a valid file or url\n"
     ]
    }
   ],
   "source": [
    "# Data ingestion via PyPDFLoader\n",
    "print(\"PyPDFLoader\")\n",
    "try:\n",
    "    py_pdf_loader = PyPDFLoader(\"data/MLE_CV_2025.pdf\")\n",
    "    py_pdf_docs = py_pdf_loader.load()\n",
    "    print(py_pdf_docs)\n",
    "    print(f\" Loaded {len(py_pdf_docs)} pages from PDF document\")\n",
    "    print(f\" Page 1 content first 100 tokens: {py_pdf_docs[0].page_content[:100]}\")\n",
    "    print(f\" Meta data: {py_pdf_docs[0].metadata}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260cac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDFLoader\n",
      "Error: File path data/MLE_CV_2025.pdf is not a valid file or url\n"
     ]
    }
   ],
   "source": [
    "# Data ingestion via PyMuPDF Loader\n",
    "print(\"PyMuPDFLoader\")\n",
    "try:\n",
    "    py_mupdf_loader = PyMuPDFLoader(\"data/MLE_CV_2025.pdf\")\n",
    "    py_mupdf_loader = py_pdf_loader.load()\n",
    "    print(py_mupdf_loader)\n",
    "    print(f\" Loaded {len(py_mupdf_loader)} pages from PDF document\")\n",
    "    print(f\" Page 1 content first 100 tokens: {py_mupdf_loader[0].page_content[:100]}\")\n",
    "    print(f\" Meta data: {py_mupdf_loader[0].metadata}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0aa7d",
   "metadata": {},
   "source": [
    "### Comparision between both loaders\n",
    "- Not much differences in our use case, our PDF file is too small and highly structured\n",
    "- PDF file contains mostly text data(non images)\n",
    "- For PDF that are standardized PDFloader will suffice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2b6fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30efbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFProcessor:\n",
    "    \"\"\"Advance PDF parsing\"\"\"\n",
    "    def __init__(self, chunk_size=500, chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"Process a PDF and return a list of chunked documents.\"\"\"\n",
    "        \n",
    "        # Load the PDF\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        \n",
    "        # Load the document and split it into chunks in one step\n",
    "        documents = loader.load_and_split(text_splitter=self.text_splitter)\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c64adcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PDFProcessor()\n",
    "pdf_document = preprocessor.process_pdf(pdf_path=\"../data/MLE_CV_2025.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46d2bd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='AI Singapore – AI Apprentice | Jan 2024 - Oct 2024\\nEnhanced clinician efficiency by 15% through an end-to-end ML pipeline (20 data sources, 4 engineers) that enabled\\nprioritization of low-confidence daily predictions.\\nEliminated the need for clinicians to manually extract symptoms from medical reports, enhancing efficiency by\\ncollaborating on fine-tuning a biomedical BERT LLM that improved feature F1 score by 25%.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='collaborating on fine-tuning a biomedical BERT LLM that improved feature F1 score by 25%.\\nDeveloped a customized date aggregation module using the Least Squares method to handle irregularities in time-\\nseries data, resulting in a 10% improvement in explainability boosting model F1-score.\\nContainerized the ML pipeline with Docker and integrated FastAPI for testing, ensuring OS-agnostic deployment and\\nrobust validation across Windows, Linux, and Mac.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='robust validation across Windows, Linux, and Mac.\\nMentored 8 junior apprentices in Neural Networks and NLP, fostering collaboration and skill development.\\nCriAT (Credit Risk Analytics Startup) – Product Analyst | Jan 2022 - Sep 2022\\nBenchmarked internal Probability of Default (PD) models against competitors (Moody’s/S&P) standards using Power\\nBI, delivering actionable insights to sales teams to improve competitive positioning.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='BI, delivering actionable insights to sales teams to improve competitive positioning.\\nEngineered Python scripts to automate data quality assurance for credit risk irregular spikes alerts, enhancing daily QA\\nefficiency by 25% and eliminating over 8 hours of manual review weekly.\\nStarted data migration analysis from Bloomberg to FactSet, validating at least 90% company coverage within product.\\nVarious Top Tier Banks & Deloitte – Private Banking Support Manager | 2016 - 2021'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Various Top Tier Banks & Deloitte – Private Banking Support Manager | 2016 - 2021\\nReduced client onboarding (KYC) turnaround from 5 days to 4 days (20%) by streamlining document verification\\nworkflows\\nManaged complex regulatory compliance processes (KYC, CDD, transaction monitoring, fraud prevention) across\\ndiverse market segments, ensuring adherence to stringent industry standards.\\nFreelance – TMT Equity Research Assistant | Jun 2018 - Aug 2019'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Freelance – TMT Equity Research Assistant | Jun 2018 - Aug 2019\\nBuilt financial models and evaluated SEC reports to forecast earnings and profitability for listed and pre-IPO\\ncompanies.\\nThong Yong Quan, Danny\\n +65 98456880 | dannyyqthong@gmail.com | LinkedIn | Portfolio \\nProgramming & ML: Python, SQL, PyTorch, Scikit-learn, FastAPI, LangChain, Deep Learning, NLP, Transformers\\nLLM Ecosystem: OpenAI, OLLAMA, GROQ, LangGraph, CrewAI, Hugging Face, RAG'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='LLM Ecosystem: OpenAI, OLLAMA, GROQ, LangGraph, CrewAI, Hugging Face, RAG\\nMLOps, Cloud: AWS (EC2, ECR), Azure (Container registry, Web App), Docker, CI/CD (GitHub Actions), MLflow, DagsHub\\nData Tools & Databases: Power BI, FAISS, Chroma, Astra DB\\nOthers: Financial Modeling (Netflix forecast), Financial Analysis, Compliance\\nAI/ML & Analytics: Associate AI Engineer (AI Singapore), Business & Data Analytics (BCG)\\nCorporate Finance Institute® (CFI): Financial Modeling & Valuation Analyst'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Corporate Finance Institute® (CFI): Financial Modeling & Valuation Analyst\\nUniversity of London: Bachelor’s Degree in Banking and Finance\\n     PROFESSIONAL EXPERIENCE\\n     PROJECTS\\n     PROFESSIONAL SUMMARY\\nAI/ML Engineer with a unique blend of expertise in Python, Deep Learning, LLMs (RAG), and MLOps, coupled with a strong\\nbackground in financial compliance and product analytics. I leverage deep analytical rigor and practical experience to develop'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='and deploy data-driven solutions, particularly in regulated environments, as demonstrated by a 15% improvement in clinical\\ndecision-making efficiency through end-to-end ML pipelines.\\nRAG-Powered Chatbot for PDF Search\\nDesigned and deployed a RAG LLM chatbot on Streamlit Cloud, saving 70% of user time with instant PDF/web\\nanswers, expanding search by 2x with Arxiv, Wikipedia, DuckDuckGo tooling.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='answers, expanding search by 2x with Arxiv, Wikipedia, DuckDuckGo tooling.\\nUtilized LangChain, GROQ, PyPDFLoader, FAISS vectorstore, and HuggingFace embeddings for efficient querying.\\nNetwork Security Phishing Project End to End Deployment\\nImplemented a traffic classifier with a 90% F1-score, which reduced analyst time by 50% for prioritizing low-confidence\\npredictions.\\nDeployed on AWS (EC2, ECR) and Azure(Azure Registry, Web App) with Docker, ensuring scalability and reliability.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-06-23T11:08:34+00:00', 'title': 'MLE_CV_2025', 'moddate': '2025-06-23T11:08:33+00:00', 'keywords': 'DAGc8XXwA3s,BAFdjIIJdRo,0', 'author': 'yong quan', 'source': '../data/MLE_CV_2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='NLP Kindle Review Sentiment Classification\\nEngineered an NLP system to classify Kindle review sentiment using Scikit-learn, NLTK, and Gensim and used\\nDocker to debug environment issues, enabling successful deployment on Streamlit Cloud.\\nAchieved 64.2% accuracy on AvgWord2Vec, outperforming BoW and TF-IDF techniques by 10%.\\nEngineered a pipeline with tokenization, stopword removal & vectorization (BoW, TF-IDF, Word2Vec, AvgWord2Vec).\\n     EDUCATION AND CERTIFICATIONS\\n     TECHNICAL SKILLS')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f205976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVParser:\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text.lower()  # normalize for matching\n",
    "\n",
    "    def parse(self) -> Dict:\n",
    "        return {\n",
    "            \"summary\": self.extract_section([\"summary\", \"profile\", \"professional summary\", \"about me\"]),\n",
    "            \"education\": self.extract_section([\"education\", \"academic\", \"academic background\", \"certification\", \"qualifications\", \"degrees\", \\\n",
    "                \"education and certification\"]),\n",
    "            \"experience\": self.extract_section([\"experience\", \"work history\", \"employment\", \"career\", \"professional experience\"]),\n",
    "            \"skills\": self.extract_section([\"skills\", \"technologies\", \"technical skills\", \"competencies\"]),\n",
    "            \"projects\": self.extract_section([\"projects\", \"project\", \"portfolio\", \"achievements\"]),\n",
    "        }\n",
    "\n",
    "    def extract_section(self, headers: list) -> str:\n",
    "        \"\"\"Return text under a section header until the next header\"\"\"\n",
    "        lines = self.text.splitlines()\n",
    "        section_lines = []\n",
    "        current_section = None\n",
    "\n",
    "        for line in lines:\n",
    "            clean_line = line.strip()\n",
    "            if not clean_line:\n",
    "                continue\n",
    "\n",
    "            # Check if the line matches a section header\n",
    "            if any(h.lower() in clean_line for h in headers):\n",
    "                current_section = True\n",
    "                continue\n",
    "\n",
    "            # Append lines under current section\n",
    "            if current_section:\n",
    "                # Stop if we hit another known section header\n",
    "                if any(h.lower() in clean_line for h in sum([v for v in [headers]], [])):\n",
    "                    break\n",
    "                section_lines.append(line)\n",
    "\n",
    "        return \"\\n\".join(section_lines).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcf4f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = PDFProcessor()\n",
    "docs = processor.process_pdf(\"../data/MLE_CV_2025.pdf\")\n",
    "full_text = \" \".join([doc.page_content for doc in docs])\n",
    "parser = CVParser(full_text)\n",
    "metadata = parser.parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d43a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'ai/ml engineer with a unique blend of expertise in python, deep learning, llms (rag), and mlops, coupled with a strong\\nbackground in financial compliance and product analytics. i leverage deep analytical rigor and practical experience to develop and deploy data-driven solutions, particularly in regulated environments, as demonstrated by a 15% improvement in clinical\\ndecision-making efficiency through end-to-end ml pipelines.\\nrag-powered chatbot for pdf search\\ndesigned and deployed a rag llm chatbot on streamlit cloud, saving 70% of user time with instant pdf/web\\nanswers, expanding search by 2x with arxiv, wikipedia, duckduckgo tooling. answers, expanding search by 2x with arxiv, wikipedia, duckduckgo tooling.\\nutilized langchain, groq, pypdfloader, faiss vectorstore, and huggingface embeddings for efficient querying.\\nnetwork security phishing project end to end deployment\\nimplemented a traffic classifier with a 90% f1-score, which reduced analyst time by 50% for prioritizing low-confidence\\npredictions.\\ndeployed on aws (ec2, ecr) and azure(azure registry, web app) with docker, ensuring scalability and reliability. nlp kindle review sentiment classification\\nengineered an nlp system to classify kindle review sentiment using scikit-learn, nltk, and gensim and used\\ndocker to debug environment issues, enabling successful deployment on streamlit cloud.\\nachieved 64.2% accuracy on avgword2vec, outperforming bow and tf-idf techniques by 10%.\\nengineered a pipeline with tokenization, stopword removal & vectorization (bow, tf-idf, word2vec, avgword2vec).\\n     education and certifications\\n     technical skills',\n",
       " 'education': 'technical skills',\n",
       " 'experience': 'projects\\n     professional summary\\nai/ml engineer with a unique blend of expertise in python, deep learning, llms (rag), and mlops, coupled with a strong\\ndecision-making efficiency through end-to-end ml pipelines.\\nrag-powered chatbot for pdf search\\ndesigned and deployed a rag llm chatbot on streamlit cloud, saving 70% of user time with instant pdf/web\\nanswers, expanding search by 2x with arxiv, wikipedia, duckduckgo tooling. answers, expanding search by 2x with arxiv, wikipedia, duckduckgo tooling.\\nutilized langchain, groq, pypdfloader, faiss vectorstore, and huggingface embeddings for efficient querying.\\nnetwork security phishing project end to end deployment\\nimplemented a traffic classifier with a 90% f1-score, which reduced analyst time by 50% for prioritizing low-confidence\\npredictions.\\ndeployed on aws (ec2, ecr) and azure(azure registry, web app) with docker, ensuring scalability and reliability. nlp kindle review sentiment classification\\nengineered an nlp system to classify kindle review sentiment using scikit-learn, nltk, and gensim and used\\ndocker to debug environment issues, enabling successful deployment on streamlit cloud.\\nachieved 64.2% accuracy on avgword2vec, outperforming bow and tf-idf techniques by 10%.\\nengineered a pipeline with tokenization, stopword removal & vectorization (bow, tf-idf, word2vec, avgword2vec).\\n     education and certifications\\n     technical skills',\n",
       " 'skills': '',\n",
       " 'projects': 'programming & ml: python, sql, pytorch, scikit-learn, fastapi, langchain, deep learning, nlp, transformers\\nllm ecosystem: openai, ollama, groq, langgraph, crewai, hugging face, rag llm ecosystem: openai, ollama, groq, langgraph, crewai, hugging face, rag\\nmlops, cloud: aws (ec2, ecr), azure (container registry, web app), docker, ci/cd (github actions), mlflow, dagshub\\ndata tools & databases: power bi, faiss, chroma, astra db\\nothers: financial modeling (netflix forecast), financial analysis, compliance\\nai/ml & analytics: associate ai engineer (ai singapore), business & data analytics (bcg)\\ncorporate finance institute® (cfi): financial modeling & valuation analyst corporate finance institute® (cfi): financial modeling & valuation analyst\\nuniversity of london: bachelor’s degree in banking and finance\\n     professional experience\\n     professional summary\\nai/ml engineer with a unique blend of expertise in python, deep learning, llms (rag), and mlops, coupled with a strong\\nbackground in financial compliance and product analytics. i leverage deep analytical rigor and practical experience to develop and deploy data-driven solutions, particularly in regulated environments, as demonstrated by a 15% improvement in clinical\\ndecision-making efficiency through end-to-end ml pipelines.\\nrag-powered chatbot for pdf search\\ndesigned and deployed a rag llm chatbot on streamlit cloud, saving 70% of user time with instant pdf/web\\nanswers, expanding search by 2x with arxiv, wikipedia, duckduckgo tooling. answers, expanding search by 2x with arxiv, wikipedia, duckduckgo tooling.\\nutilized langchain, groq, pypdfloader, faiss vectorstore, and huggingface embeddings for efficient querying.\\nimplemented a traffic classifier with a 90% f1-score, which reduced analyst time by 50% for prioritizing low-confidence\\npredictions.\\ndeployed on aws (ec2, ecr) and azure(azure registry, web app) with docker, ensuring scalability and reliability. nlp kindle review sentiment classification\\nengineered an nlp system to classify kindle review sentiment using scikit-learn, nltk, and gensim and used\\ndocker to debug environment issues, enabling successful deployment on streamlit cloud.\\nachieved 64.2% accuracy on avgword2vec, outperforming bow and tf-idf techniques by 10%.\\nengineered a pipeline with tokenization, stopword removal & vectorization (bow, tf-idf, word2vec, avgword2vec).\\n     education and certifications\\n     technical skills'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57158d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "class JDSectionExtractor:\n",
    "    \"\"\"Extract sections from a job description without regex\"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "\n",
    "        # Section groups with flexible headers\n",
    "        self.section_groups = {\n",
    "            \"company_info\": [\"about us\", \"company\", \"who we are\"],\n",
    "            \"role\": [\"your role\", \"role\", \"position overview\", \"job purpose\", \"what you will be doing\"],\n",
    "            \"responsibilities\": [\"key responsibilities\", \"responsibilities\", \"duties\", \"what you'll do\"],\n",
    "            \"qualifications\": [\"qualifications\", \"requirements\", \"who you are\", \"skills\"],\n",
    "            \"benefits\": [\"what we offer\", \"perks\", \"compensation\", \"why join us\", \"benefits\"],\n",
    "        }\n",
    "\n",
    "    def extract_sections(self) -> Dict[str, str]:\n",
    "        sections: Dict[str, List[str]] = {}\n",
    "        current_section = None\n",
    "\n",
    "        # Split JD into lines\n",
    "        lines = self.text.splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            clean_line = line.strip()\n",
    "\n",
    "            if not clean_line:  # skip empty lines\n",
    "                continue\n",
    "\n",
    "            # Check if the line matches any known section header\n",
    "            lowered = clean_line.lower()\n",
    "            found_section = None\n",
    "            for section_name, headers in self.section_groups.items():\n",
    "                if any(lowered.startswith(h) for h in headers):\n",
    "                    found_section = section_name\n",
    "                    break\n",
    "\n",
    "            # If we hit a header, start a new section\n",
    "            if found_section:\n",
    "                current_section = found_section\n",
    "                sections[current_section] = []\n",
    "            elif current_section:\n",
    "                # Append line under the current section\n",
    "                sections[current_section].append(clean_line)\n",
    "\n",
    "        # Join lines back into paragraphs\n",
    "        return {k: \"\\n\".join(v).strip() for k, v in sections.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6260fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample via LinkedIn\n",
    "\n",
    "jd_text = \"\"\"About the job\n",
    "About us \n",
    "\n",
    "RE-LIVE is a next-generation real estate insights platform focused on streamlining and modernizing the valuation process for multiple commercial and residential property types. Our mission is to enable access to property intelligence through clean UI, data integrations, and dynamic reporting, with a strong emphasis on usability and accuracy. This role will contribute directly to transforming how real estate is evaluated and reported.\n",
    "\n",
    "\n",
    "Your Role\n",
    "\n",
    "As our Machine Learning Engineer, you'll spearhead the development of predictive models for real estate pricing, incorporating a rich variety of structured and unstructured data sources. You’ll work across the full ML lifecycle—from data acquisition and preprocessing to modeling, evaluation, and front-end visualization.\n",
    "\n",
    "\n",
    "Key Responsibilities\n",
    "\n",
    "Develop and deploy machine learning models to predict real estate prices and investment potential.\n",
    "Identify and transform relevant signals from heterogeneous datasets to support accurate property value predictions.\n",
    "Conduct NLP-based analysis of textual data (e.g. user reviews, market reports, real estate articles) to enrich model inputs.\n",
    "Collect, clean, merge, and manage large and diverse datasets from APIs, web scraping, public sources, and commercial databases.\n",
    "Design and implement interactive dashboards to visualize trends, model predictions, and insights in a user-friendly manner.\n",
    "Collaborate with valuation experts and product designers to integrate insights into our platform.\n",
    "\n",
    "\n",
    "Qualifications\n",
    "\n",
    "Bachelor's or Master's degree in Computer Science, Data Science, or related field.\n",
    "Strong academic background or demonstrable track record of high-impact, self-driven work in data science or machine learning.\n",
    "Strong proficiency in Python and data science libraries like Pandas, Scikit-learn, NumPy.\n",
    "Experience with deep learning frameworks (e.g. PyTorch, TensorFlow) for regression and NLP tasks.\n",
    "Hands-on experience building interactive dashboards using Dash, Plotly, or Streamlit.\n",
    "Familiarity with geospatial data and tools like GeoPandas, Shapely, or Kepler.gl is a plus.\n",
    "Bonus points for knowledge of PostgreSQL/PostGIS, Elasticsearch, or LLMs for contextual insights.\n",
    "\n",
    "\n",
    "What We Offer\n",
    "\n",
    "Opportunity to build and scale a product that will redefine real estate investing in Asia and beyond.\n",
    "Flexible working hours.\n",
    "Collaborative, innovation-driven environment with direct access to decision-makers.\n",
    "Competitive compensation and performance incentives.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bbbde00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COMPANY_INFO ---\n",
      "RE-LIVE is a next-generation real estate insights platform focused on streamlining and modernizing the valuation process for multiple commercial and residential property types. Our mission is to enable access to property intelligence through clean UI, data integrations, and dynamic reporting, with a strong emphasis on usability and accuracy. This role will contribute directly to transforming how real estate is evaluated and reported.\n",
      "\n",
      "--- ROLE ---\n",
      "As our Machine Learning Engineer, you'll spearhead the development of predictive models for real estate pricing, incorporating a rich variety of structured and unstructured data sources. You’ll work across the full ML lifecycle—from data acquisition and preprocessing to modeling, evaluation, and front-end visualization.\n",
      "\n",
      "--- RESPONSIBILITIES ---\n",
      "Develop and deploy machine learning models to predict real estate prices and investment potential.\n",
      "Identify and transform relevant signals from heterogeneous datasets to support accurate property value predictions.\n",
      "Conduct NLP-based analysis of textual data (e.g. user reviews, market reports, real estate articles) to enrich model inputs.\n",
      "Collect, clean, merge, and manage large and diverse datasets from APIs, web scraping, public sources, and commercial databases.\n",
      "Design and implement interactive dashboards to visualize trends, model predictions, and insights in a user-friendly manner.\n",
      "Collaborate with valuation experts and product designers to integrate insights into our platform.\n",
      "\n",
      "--- QUALIFICATIONS ---\n",
      "Bachelor's or Master's degree in Computer Science, Data Science, or related field.\n",
      "Strong academic background or demonstrable track record of high-impact, self-driven work in data science or machine learning.\n",
      "Strong proficiency in Python and data science libraries like Pandas, Scikit-learn, NumPy.\n",
      "Experience with deep learning frameworks (e.g. PyTorch, TensorFlow) for regression and NLP tasks.\n",
      "Hands-on experience building interactive dashboards using Dash, Plotly, or Streamlit.\n",
      "Familiarity with geospatial data and tools like GeoPandas, Shapely, or Kepler.gl is a plus.\n",
      "Bonus points for knowledge of PostgreSQL/PostGIS, Elasticsearch, or LLMs for contextual insights.\n",
      "\n",
      "--- BENEFITS ---\n",
      "Opportunity to build and scale a product that will redefine real estate investing in Asia and beyond.\n",
      "Flexible working hours.\n",
      "Collaborative, innovation-driven environment with direct access to decision-makers.\n",
      "Competitive compensation and performance incentives.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extractor = JDSectionExtractor(jd_text)\n",
    "jd_sections = extractor.extract_sections()\n",
    "\n",
    "for title, content in jd_sections.items():\n",
    "    print(f\"--- {title.upper()} ---\\n{content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d862482",
   "metadata": {},
   "source": [
    "### Pros and Cons of parsers\n",
    "#### Parsing of CV\n",
    "- Given a structured CV, parsing will work great however if the CV has different style and canvas, manual parsing will bound to fail\n",
    "- Move on to use LLM to parse the whole CV into a JSON output\n",
    "\n",
    "#### Parsing of JD\n",
    "- Works well as expected, as JD typcially only consists highly smiliar make up terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e674bd",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- In this use case, CVS are typically 1-2 pages which will usually not hit token limit, therefore instead of chunking we can actually let LLM handles the full text in one prompt\n",
    "- Have to carefully engineer good prompts to extract necessarily information headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0489ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-agent (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
