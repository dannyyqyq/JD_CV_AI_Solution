{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed08499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b492c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root 'c:\\Users\\yq198\\Desktop\\DANNY_AI\\CV_Agent' successfully added to Python path\n"
     ]
    }
   ],
   "source": [
    "def check_project_root():\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    if project_root not in sys.path:\n",
    "        raise ImportError(f\"Project root '{project_root}' not found in Python path\")\n",
    "    print(f\"Project root '{project_root}' successfully added to Python path\")\n",
    "\n",
    "try:\n",
    "    check_project_root()\n",
    "except ImportError as e:\n",
    "\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "544d09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_deepseek_model = \"deepseek-r1:latest\"\n",
    "llm_qwen_model = \"qwen3:latest\"\n",
    "llm_gemma3_model = \"gemma3:270m\"\n",
    "# Initialize the Ollama model\n",
    "llm_deepseek = OllamaLLM(model=llm_deepseek_model) \n",
    "llm_qwen = OllamaLLM(model=llm_qwen_model) \n",
    "llm_gemma3 = OllamaLLM(model=llm_gemma3_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cdf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document_ingestion.pdf_processor import PDFProcessor\n",
    "\n",
    "processor = PDFProcessor()\n",
    "docs = processor.process_pdf(\"../data/MLE_CV_2025.pdf\")\n",
    "cv_text = \" \".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b80608e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Singapore – AI Apprentice | Jan 2024 - Oct 2024\n",
      "Enhanced clinician efficiency by 15% through an end-to-end ML pipeline (20 data sources, 4 engineers) that enabled\n",
      "prioritization of low-confidence daily predictions.\n",
      "Eliminated the need for clinicians to manually extract symptoms from medical reports, enhancing efficiency by\n",
      "collaborating on fine-tuning a biomedical BERT LLM that improved feature F1 score by 25%. collaborating on fine-tuning a biomedical BERT LLM that improved feature F1 score by 25%.\n",
      "Developed a customized date aggregation module using the Least Squares method to handle irregularities in time-\n",
      "series data, resulting in a 10% improvement in explainability boosting model F1-score.\n",
      "Containerized the ML pipeline with Docker and integrated FastAPI for testing, ensuring OS-agnostic deployment and\n",
      "robust validation across Windows, Linux, and Mac. robust validation across Windows, Linux, and Mac.\n",
      "Mentored 8 junior apprentices in Neural Networks and NLP, fostering collaboration and skill development.\n",
      "CriAT (Credit Risk Analytics Startup) – Product Analyst | Jan 2022 - Sep 2022\n",
      "Benchmarked internal Probability of Default (PD) models against competitors (Moody’s/S&P) standards using Power\n",
      "BI, delivering actionable insights to sales teams to improve competitive positioning. BI, delivering actionable insights to sales teams to improve competitive positioning.\n",
      "Engineered Python scripts to automate data quality assurance for credit risk irregular spikes alerts, enhancing daily QA\n",
      "efficiency by 25% and eliminating over 8 hours of manual review weekly.\n",
      "Started data migration analysis from Bloomberg to FactSet, validating at least 90% company coverage within product.\n",
      "Various Top Tier Banks & Deloitte – Private Banking Support Manager | 2016 - 2021 Various Top Tier Banks & Deloitte – Private Banking Support Manager | 2016 - 2021\n",
      "Reduced client onboarding (KYC) turnaround from 5 days to 4 days (20%) by streamlining document verification\n",
      "workflows\n",
      "Managed complex regulatory compliance processes (KYC, CDD, transaction monitoring, fraud prevention) across\n",
      "diverse market segments, ensuring adherence to stringent industry standards.\n",
      "Freelance – TMT Equity Research Assistant | Jun 2018 - Aug 2019 Freelance – TMT Equity Research Assistant | Jun 2018 - Aug 2019\n",
      "Built financial models and evaluated SEC reports to forecast earnings and profitability for listed and pre-IPO\n",
      "companies.\n",
      "Thong Yong Quan, Danny\n",
      " +65 98456880 | dannyyqthong@gmail.com | LinkedIn | Portfolio \n",
      "Programming & ML: Python, SQL, PyTorch, Scikit-learn, FastAPI, LangChain, Deep Learning, NLP, Transformers\n",
      "LLM Ecosystem: OpenAI, OLLAMA, GROQ, LangGraph, CrewAI, Hugging Face, RAG LLM Ecosystem: OpenAI, OLLAMA, GROQ, LangGraph, CrewAI, Hugging Face, RAG\n",
      "MLOps, Cloud: AWS (EC2, ECR), Azure (Container registry, Web App), Docker, CI/CD (GitHub Actions), MLflow, DagsHub\n",
      "Data Tools & Databases: Power BI, FAISS, Chroma, Astra DB\n",
      "Others: Financial Modeling (Netflix forecast), Financial Analysis, Compliance\n",
      "AI/ML & Analytics: Associate AI Engineer (AI Singapore), Business & Data Analytics (BCG)\n",
      "Corporate Finance Institute® (CFI): Financial Modeling & Valuation Analyst Corporate Finance Institute® (CFI): Financial Modeling & Valuation Analyst\n",
      "University of London: Bachelor’s Degree in Banking and Finance\n",
      "     PROFESSIONAL EXPERIENCE\n",
      "     PROJECTS\n",
      "     PROFESSIONAL SUMMARY\n",
      "AI/ML Engineer with a unique blend of expertise in Python, Deep Learning, LLMs (RAG), and MLOps, coupled with a strong\n",
      "background in financial compliance and product analytics. I leverage deep analytical rigor and practical experience to develop and deploy data-driven solutions, particularly in regulated environments, as demonstrated by a 15% improvement in clinical\n",
      "decision-making efficiency through end-to-end ML pipelines.\n",
      "RAG-Powered Chatbot for PDF Search\n",
      "Designed and deployed a RAG LLM chatbot on Streamlit Cloud, saving 70% of user time with instant PDF/web\n",
      "answers, expanding search by 2x with Arxiv, Wikipedia, DuckDuckGo tooling. answers, expanding search by 2x with Arxiv, Wikipedia, DuckDuckGo tooling.\n",
      "Utilized LangChain, GROQ, PyPDFLoader, FAISS vectorstore, and HuggingFace embeddings for efficient querying.\n",
      "Network Security Phishing Project End to End Deployment\n",
      "Implemented a traffic classifier with a 90% F1-score, which reduced analyst time by 50% for prioritizing low-confidence\n",
      "predictions.\n",
      "Deployed on AWS (EC2, ECR) and Azure(Azure Registry, Web App) with Docker, ensuring scalability and reliability. NLP Kindle Review Sentiment Classification\n",
      "Engineered an NLP system to classify Kindle review sentiment using Scikit-learn, NLTK, and Gensim and used\n",
      "Docker to debug environment issues, enabling successful deployment on Streamlit Cloud.\n",
      "Achieved 64.2% accuracy on AvgWord2Vec, outperforming BoW and TF-IDF techniques by 10%.\n",
      "Engineered a pipeline with tokenization, stopword removal & vectorization (BoW, TF-IDF, Word2Vec, AvgWord2Vec).\n",
      "     EDUCATION AND CERTIFICATIONS\n",
      "     TECHNICAL SKILLS\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "print(cv_text)\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eebc077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_prompt = f\"\"\"\n",
    "You are a CV parsing assistant. \n",
    "\n",
    "Extract the following fields from this CV text and return ONLY valid JSON inside a fenced code block (```json ... ```). \n",
    "Do not output anything else.\n",
    "\n",
    "Schema:\n",
    "```json\n",
    "{{\n",
    "  \"summary\": \"As per below guidelines\",\n",
    "  \"education\": [\n",
    "    {{\n",
    "      \"institution\": \"As per below guidelines\",\n",
    "      \"degree\": \"As per below guidelines\",\n",
    "      \"start_date\": \"mm/yyyy or yyyy\",\n",
    "      \"end_date\": \"mm/yyyy or yyyy\"\n",
    "    }}\n",
    "  ],\n",
    "  \"experience\": [\n",
    "    {{\n",
    "      \"job_title\": \"As per below guidelines\",\n",
    "      \"company\": \"As per below guidelines (leave empty if freelance/self-employed)\",\n",
    "      \"start_date\": \"mm/yyyy or yyyy or yyyy/mm\",\n",
    "      \"end_date\": \"mm/yyyy or yyyy or yyyy/mm\",\n",
    "      \"description\": \"As per below guidelines\"\n",
    "    }}\n",
    "  ],\n",
    "  \"skills\": [\"As per below guidelines\"],\n",
    "  \"projects\": [\n",
    "    {{\n",
    "      \"project_title\": \"As per below guidelines\",\n",
    "      \"description\": \"As per below guidelines\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Use the following guidelines:\n",
    "\n",
    "1. \"summary\": include text corresponding to [\"summary\", \"profile\", \"professional summary\", \"about me\"]\n",
    "2. \"education\": include text corresponding to [\"education\", \"academic\", \"certification\", \"degrees\"]\n",
    "3. \"experience\": include text corresponding to [\"experience\", \"work history\", \"employment\", \"career\", \"professional experience\"]\n",
    "  - If a role is freelance/self-employed\n",
    "  - output the entire title in \"job_title\"\n",
    "  - leave \"company\" as an empty string \"\"\n",
    "  - otherwise \"company\" should always have a input\n",
    "4. \"skills\": include text corresponding to [\"skills\", \"technologies\", \"technical skills\", \"competencies\"]\n",
    "5. \"projects\": include text corresponding to [\"projects\", \"project\", \"portfolio\", \"achievements\"]\n",
    "\n",
    "Return fully populated JSON. Do not use ellipses `...`. If a section is missing, return [] or \"\".\n",
    "    \n",
    "    \n",
    "Here is the CV text:\n",
    "\n",
    "\"{cv_text}\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62642001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse JSON. Raw output:\n",
      "```json\n",
      "{\n",
      "  \"summary\": \"As per below guidelines\",\n",
      "  \"education\": [\n",
      "  {\n",
      "    \"institution\": \"As per below guidelines\",\n",
      "    \"degree\": \"As per below guidelines\",\n",
      "    \"start_date\": \"mm/yyyy or yyyy\",\n",
      "    \"end_date\": \"mm/yyyy or yyyy\"\n",
      "  },\n",
      "  {\n",
      "    \"institution\": \"As per below guidelines\",\n",
      "    \"degree\": \"As per below guidelines\",\n",
      "    \"start_date\": \"mm/yyyy or yyyy\",\n",
      "    \"end_date\": \"mm/yyyy or yyyy\"\n",
      "  }\n",
      " ],\n",
      "  \"experience\": [\n",
      "  {\n",
      "    \"job_title\": \"As per below guidelines\",\n",
      "    \"company\": \"As per below guidelines (leave empty if freelance/self-employed)\",\n",
      "    \"start_date\": \"mm/yyyy or yyyy or yyyy/mm\",\n",
      "    \"end_date\": \"mm/yyyy or yyyy or yyyy/mm\",\n",
      "    \"description\": \"As per below guidelines\"\n",
      "  }\n",
      " ],\n",
      "  \"skills\": [\"As per below guidelines\"],\n",
      "  \"projects\": [\n",
      "  {\n",
      "    \"project_title\": \"As per below guidelines\",\n",
      "    \"description\": \"As per below guidelines\"\n",
      "  }\n",
      " ],\n",
      "  \"projects\": [\n",
      "  {\n",
      "    \"project_title\": \"As per below guidelines\",\n",
      "    \"description\": \"As per below guidelines\"\n",
      "  }\n",
      " ],\n",
      "  \"documentation\": \"As per below guidelines\"\n",
      "}\n",
      "```\n",
      "('```json\\n'\n",
      " '{\\n'\n",
      " '  \"summary\": \"As per below guidelines\",\\n'\n",
      " '  \"education\": [\\n'\n",
      " '  {\\n'\n",
      " '    \"institution\": \"As per below guidelines\",\\n'\n",
      " '    \"degree\": \"As per below guidelines\",\\n'\n",
      " '    \"start_date\": \"mm/yyyy or yyyy\",\\n'\n",
      " '    \"end_date\": \"mm/yyyy or yyyy\"\\n'\n",
      " '  },\\n'\n",
      " '  {\\n'\n",
      " '    \"institution\": \"As per below guidelines\",\\n'\n",
      " '    \"degree\": \"As per below guidelines\",\\n'\n",
      " '    \"start_date\": \"mm/yyyy or yyyy\",\\n'\n",
      " '    \"end_date\": \"mm/yyyy or yyyy\"\\n'\n",
      " '  }\\n'\n",
      " ' ],\\n'\n",
      " '  \"experience\": [\\n'\n",
      " '  {\\n'\n",
      " '    \"job_title\": \"As per below guidelines\",\\n'\n",
      " '    \"company\": \"As per below guidelines (leave empty if '\n",
      " 'freelance/self-employed)\",\\n'\n",
      " '    \"start_date\": \"mm/yyyy or yyyy or yyyy/mm\",\\n'\n",
      " '    \"end_date\": \"mm/yyyy or yyyy or yyyy/mm\",\\n'\n",
      " '    \"description\": \"As per below guidelines\"\\n'\n",
      " '  }\\n'\n",
      " ' ],\\n'\n",
      " '  \"skills\": [\"As per below guidelines\"],\\n'\n",
      " '  \"projects\": [\\n'\n",
      " '  {\\n'\n",
      " '    \"project_title\": \"As per below guidelines\",\\n'\n",
      " '    \"description\": \"As per below guidelines\"\\n'\n",
      " '  }\\n'\n",
      " ' ],\\n'\n",
      " '  \"projects\": [\\n'\n",
      " '  {\\n'\n",
      " '    \"project_title\": \"As per below guidelines\",\\n'\n",
      " '    \"description\": \"As per below guidelines\"\\n'\n",
      " '  }\\n'\n",
      " ' ],\\n'\n",
      " '  \"documentation\": \"As per below guidelines\"\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "cv_parsed_text_gemma = llm_gemma3.invoke(cv_prompt)\n",
    "\n",
    "# Inspect the parsed CV\n",
    "pprint.pprint(cv_parsed_text_gemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "934ffeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse JSON. Raw output:\n",
      "<think>\n",
      "Okay, let's tackle this CV extraction task. The user wants me to parse the given CV text into a specific JSON format following their guidelines.\n",
      "\n",
      "First, I need to identify sections matching each field in the schema from the provided CV content. Looking at the text:\n",
      "\n",
      "- **Summary/Profile**: There are multiple mentions like \"Professional Summary,\" \"Associate AI Engineer,\" etc., so I'll compile that.\n",
      "  \n",
      "- **Education/Academic/Certification**: The user mentioned University of London and Corporate Finance Institute, but details are incomplete. Since no specific dates or courses were provided beyond the degrees, education entries should be left with placeholders.\n",
      "\n",
      "- **Experience/Work History**: There's clear work experience listed under different companies/titles with start and end dates. I need to parse each role carefully:\n",
      "  - The first entry is \"AI Singapore – AI Apprentice\" from Jan 2024 to Oct 2024.\n",
      "  - Then \"CriAT (Credit Risk Analytics Startup) – Product Analyst\" from Jan 2022 to Sep 2022.\n",
      "  - Next, a role labeled as \"Various Top Tier Banks & Deloitte,\" which seems like multiple employers but lacks company names. I'll treat this as one entry with the given date range.\n",
      "  - The fourth role is freelance at \"Freelance – TMT Equity Research Assistant\" from Jun 2018 to Aug 2019.\n",
      "\n",
      "For each experience, if it's freelance, \"company\" should be empty except for job_title. Otherwise, provide company details and ensure start/end dates are formatted correctly (mm/yyyy or yyyy).\n",
      "\n",
      "- **Skills**: The skills listed at the bottom correspond exactly to technical competencies mentioned by the user: Programming & ML, LLM Ecosystem, MLOps/Cloud, Data Tools/Databases, Others.\n",
      "\n",
      "- **Projects**: There's a section titled \"PROJECTS\" with three projects. Each should have its title and description extracted accurately without truncation or placeholders.\n",
      "\n",
      "Now, checking for completeness:\n",
      "  - Summary exists.\n",
      "  - Education has two entries but lacks specific details beyond the degree type; I'll use generic placeholders as per instructions.\n",
      "  - Experience is well-structured with all four roles identified.\n",
      "  - Skills are comprehensive.\n",
      "  - Projects have titles and descriptions provided.\n",
      "\n",
      "I should ensure that dates follow either mm/yyyy or yyyy format. For example, \"Jan 2024\" becomes \"01/2024\". Freelance entries don't list a company in the JSON, just the job title with empty company field.\n",
      "\n",
      "The user might be testing how well I can parse unstructured data into structured formats without missing any details or making incorrect assumptions. They likely want to automate CV processing and ensure accuracy for various fields like education, experience, skills, etc., possibly for recruitment tools or personal use.\n",
      "</think>\n",
      "```json\n",
      "{\n",
      "  \"summary\": \"AI/ML Engineer with a unique blend of expertise in Python, Deep Learning, LLMs (RAG), and MLOps, coupled with a strong background in financial compliance and product analytics. I leverage deep analytical rigor and practical experience to develop and deploy data-driven solutions, particularly in regulated environments, as demonstrated by a 15% improvement in clinical decision-making efficiency through end-to-end ML pipelines.\",\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"institution\": \"\",\n",
      "      \"degree\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"institution\": \"University of London\",\n",
      "      \"degree\": \"Bachelor’s Degree in Banking and Finance\"\n",
      "    }\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"job_title\": \"AI Singapore – Associate AI Engineer (Product)\",\n",
      "      \"company\": \"AI/ML & Analytics\",\n",
      "      \"start_date\": \"01/2024\",\n",
      "      \"end_date\": \"\",\n",
      "      \"description\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"AI Singapore – Associate AI Engineer (Technical Skills)\",\n",
      "      \"company\": \"\",\n",
      "      \"start_date\": \"01/2024\",\n",
      "      \"end_date\": \"\",\n",
      "      \"description\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    \"Python\", \n",
      "    \"SQL\", \n",
      "    \"PyTorch\", \n",
      "    \"Scikit-learn\", \n",
      "    \"FastAPI\", \n",
      "    \"LangChain\", \n",
      "    \"Deep Learning\", \n",
      "    \"NLP\", \n",
      "    \"Transformers\",\n",
      "    \"OpenAI\", \n",
      "    \"OLLAMA\", \n",
      "    \"GROQ\", \n",
      "    \"LangGraph\", \n",
      "    \"CrewAI\", \n",
      "    \"Hugging Face\", \n",
      "    \"RAG\",\n",
      "    \"AWS (EC2, ECR)\",\n",
      "    \"Azure (Container registry, Web App)\",\n",
      "    \"Docker\",\n",
      "    \"CI/CD (GitHub Actions)\",\n",
      "    \"MLflow\",\n",
      "    \"DagsHub\",\n",
      "    \"Power BI\",\n",
      "    \"FAISS\",\n",
      "    \"Chroma\",\n",
      "    \"Astra DB\",\n",
      "    \"Financial Modeling (Netflix forecast)\",\n",
      "    \"Financial Analysis\",\n",
      "    \"Compliance\"\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"project_title\": \"RAG-Powered Chatbot for PDF Search\",\n",
      "      \"description\": \"Designed and deployed a RAG LLM chatbot on Streamlit Cloud, saving 70% of user time with instant PDF/web answers, expanding search by 2x with Arxiv, Wikipedia, DuckDuckGo tooling. Utilized LangChain, GROQ, PyPDFLoader, FAISS vectorstore, and HuggingFace embeddings for efficient querying.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_title\": \"Network Security Phishing Project End to End Deployment\",\n",
      "      \"description\": \"Implemented a traffic classifier with a 90% F1-score, which reduced analyst time by 50% for prioritizing low-confidence predictions. Deployed on AWS (EC2, ECR) and Azure(Azure Registry, Web App) with Docker, ensuring scalability and reliability.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_title\": \"NLP Kindle Review Sentiment Classification\",\n",
      "      \"description\": \"Engineered an NLP system to classify Kindle review sentiment using Scikit-learn, NLTK, and Gensim and used Docker to debug environment issues, enabling successful deployment on Streamlit Cloud. Achieved 64.2% accuracy on AvgWord2Vec, outperforming BoW and TF-IDF techniques by 10%. Engineered a pipeline with tokenization, stopword removal & vectorization (BoW, TF-IDF, Word2Vec, AvgWord2Vec).\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "('<think>\\n'\n",
      " \"Okay, let's tackle this CV extraction task. The user wants me to parse the \"\n",
      " 'given CV text into a specific JSON format following their guidelines.\\n'\n",
      " '\\n'\n",
      " 'First, I need to identify sections matching each field in the schema from '\n",
      " 'the provided CV content. Looking at the text:\\n'\n",
      " '\\n'\n",
      " '- **Summary/Profile**: There are multiple mentions like \"Professional '\n",
      " 'Summary,\" \"Associate AI Engineer,\" etc., so I\\'ll compile that.\\n'\n",
      " '  \\n'\n",
      " '- **Education/Academic/Certification**: The user mentioned University of '\n",
      " 'London and Corporate Finance Institute, but details are incomplete. Since no '\n",
      " 'specific dates or courses were provided beyond the degrees, education '\n",
      " 'entries should be left with placeholders.\\n'\n",
      " '\\n'\n",
      " \"- **Experience/Work History**: There's clear work experience listed under \"\n",
      " 'different companies/titles with start and end dates. I need to parse each '\n",
      " 'role carefully:\\n'\n",
      " '  - The first entry is \"AI Singapore – AI Apprentice\" from Jan 2024 to Oct '\n",
      " '2024.\\n'\n",
      " '  - Then \"CriAT (Credit Risk Analytics Startup) – Product Analyst\" from Jan '\n",
      " '2022 to Sep 2022.\\n'\n",
      " '  - Next, a role labeled as \"Various Top Tier Banks & Deloitte,\" which seems '\n",
      " \"like multiple employers but lacks company names. I'll treat this as one \"\n",
      " 'entry with the given date range.\\n'\n",
      " '  - The fourth role is freelance at \"Freelance – TMT Equity Research '\n",
      " 'Assistant\" from Jun 2018 to Aug 2019.\\n'\n",
      " '\\n'\n",
      " 'For each experience, if it\\'s freelance, \"company\" should be empty except '\n",
      " 'for job_title. Otherwise, provide company details and ensure start/end dates '\n",
      " 'are formatted correctly (mm/yyyy or yyyy).\\n'\n",
      " '\\n'\n",
      " '- **Skills**: The skills listed at the bottom correspond exactly to '\n",
      " 'technical competencies mentioned by the user: Programming & ML, LLM '\n",
      " 'Ecosystem, MLOps/Cloud, Data Tools/Databases, Others.\\n'\n",
      " '\\n'\n",
      " '- **Projects**: There\\'s a section titled \"PROJECTS\" with three projects. '\n",
      " 'Each should have its title and description extracted accurately without '\n",
      " 'truncation or placeholders.\\n'\n",
      " '\\n'\n",
      " 'Now, checking for completeness:\\n'\n",
      " '  - Summary exists.\\n'\n",
      " '  - Education has two entries but lacks specific details beyond the degree '\n",
      " \"type; I'll use generic placeholders as per instructions.\\n\"\n",
      " '  - Experience is well-structured with all four roles identified.\\n'\n",
      " '  - Skills are comprehensive.\\n'\n",
      " '  - Projects have titles and descriptions provided.\\n'\n",
      " '\\n'\n",
      " 'I should ensure that dates follow either mm/yyyy or yyyy format. For '\n",
      " 'example, \"Jan 2024\" becomes \"01/2024\". Freelance entries don\\'t list a '\n",
      " 'company in the JSON, just the job title with empty company field.\\n'\n",
      " '\\n'\n",
      " 'The user might be testing how well I can parse unstructured data into '\n",
      " 'structured formats without missing any details or making incorrect '\n",
      " 'assumptions. They likely want to automate CV processing and ensure accuracy '\n",
      " 'for various fields like education, experience, skills, etc., possibly for '\n",
      " 'recruitment tools or personal use.\\n'\n",
      " '</think>\\n'\n",
      " '```json\\n'\n",
      " '{\\n'\n",
      " '  \"summary\": \"AI/ML Engineer with a unique blend of expertise in Python, '\n",
      " 'Deep Learning, LLMs (RAG), and MLOps, coupled with a strong background in '\n",
      " 'financial compliance and product analytics. I leverage deep analytical rigor '\n",
      " 'and practical experience to develop and deploy data-driven solutions, '\n",
      " 'particularly in regulated environments, as demonstrated by a 15% improvement '\n",
      " 'in clinical decision-making efficiency through end-to-end ML pipelines.\",\\n'\n",
      " '  \"education\": [\\n'\n",
      " '    {\\n'\n",
      " '      \"institution\": \"\",\\n'\n",
      " '      \"degree\": \"\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"institution\": \"University of London\",\\n'\n",
      " '      \"degree\": \"Bachelor’s Degree in Banking and Finance\"\\n'\n",
      " '    }\\n'\n",
      " '  ],\\n'\n",
      " '  \"experience\": [\\n'\n",
      " '    {\\n'\n",
      " '      \"job_title\": \"AI Singapore – Associate AI Engineer (Product)\",\\n'\n",
      " '      \"company\": \"AI/ML & Analytics\",\\n'\n",
      " '      \"start_date\": \"01/2024\",\\n'\n",
      " '      \"end_date\": \"\",\\n'\n",
      " '      \"description\": \"\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"job_title\": \"AI Singapore – Associate AI Engineer (Technical '\n",
      " 'Skills)\",\\n'\n",
      " '      \"company\": \"\",\\n'\n",
      " '      \"start_date\": \"01/2024\",\\n'\n",
      " '      \"end_date\": \"\",\\n'\n",
      " '      \"description\": \"\"\\n'\n",
      " '    }\\n'\n",
      " '  ],\\n'\n",
      " '  \"skills\": [\\n'\n",
      " '    \"Python\", \\n'\n",
      " '    \"SQL\", \\n'\n",
      " '    \"PyTorch\", \\n'\n",
      " '    \"Scikit-learn\", \\n'\n",
      " '    \"FastAPI\", \\n'\n",
      " '    \"LangChain\", \\n'\n",
      " '    \"Deep Learning\", \\n'\n",
      " '    \"NLP\", \\n'\n",
      " '    \"Transformers\",\\n'\n",
      " '    \"OpenAI\", \\n'\n",
      " '    \"OLLAMA\", \\n'\n",
      " '    \"GROQ\", \\n'\n",
      " '    \"LangGraph\", \\n'\n",
      " '    \"CrewAI\", \\n'\n",
      " '    \"Hugging Face\", \\n'\n",
      " '    \"RAG\",\\n'\n",
      " '    \"AWS (EC2, ECR)\",\\n'\n",
      " '    \"Azure (Container registry, Web App)\",\\n'\n",
      " '    \"Docker\",\\n'\n",
      " '    \"CI/CD (GitHub Actions)\",\\n'\n",
      " '    \"MLflow\",\\n'\n",
      " '    \"DagsHub\",\\n'\n",
      " '    \"Power BI\",\\n'\n",
      " '    \"FAISS\",\\n'\n",
      " '    \"Chroma\",\\n'\n",
      " '    \"Astra DB\",\\n'\n",
      " '    \"Financial Modeling (Netflix forecast)\",\\n'\n",
      " '    \"Financial Analysis\",\\n'\n",
      " '    \"Compliance\"\\n'\n",
      " '  ],\\n'\n",
      " '  \"projects\": [\\n'\n",
      " '    {\\n'\n",
      " '      \"project_title\": \"RAG-Powered Chatbot for PDF Search\",\\n'\n",
      " '      \"description\": \"Designed and deployed a RAG LLM chatbot on Streamlit '\n",
      " 'Cloud, saving 70% of user time with instant PDF/web answers, expanding '\n",
      " 'search by 2x with Arxiv, Wikipedia, DuckDuckGo tooling. Utilized LangChain, '\n",
      " 'GROQ, PyPDFLoader, FAISS vectorstore, and HuggingFace embeddings for '\n",
      " 'efficient querying.\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"project_title\": \"Network Security Phishing Project End to End '\n",
      " 'Deployment\",\\n'\n",
      " '      \"description\": \"Implemented a traffic classifier with a 90% F1-score, '\n",
      " 'which reduced analyst time by 50% for prioritizing low-confidence '\n",
      " 'predictions. Deployed on AWS (EC2, ECR) and Azure(Azure Registry, Web App) '\n",
      " 'with Docker, ensuring scalability and reliability.\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"project_title\": \"NLP Kindle Review Sentiment Classification\",\\n'\n",
      " '      \"description\": \"Engineered an NLP system to classify Kindle review '\n",
      " 'sentiment using Scikit-learn, NLTK, and Gensim and used Docker to debug '\n",
      " 'environment issues, enabling successful deployment on Streamlit Cloud. '\n",
      " 'Achieved 64.2% accuracy on AvgWord2Vec, outperforming BoW and TF-IDF '\n",
      " 'techniques by 10%. Engineered a pipeline with tokenization, stopword removal '\n",
      " '& vectorization (BoW, TF-IDF, Word2Vec, AvgWord2Vec).\"\\n'\n",
      " '    }\\n'\n",
      " '  ]\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "cv_parsed_text_deepseek = llm_deepseek.invoke(cv_prompt)\n",
    "\n",
    "# Inspect the parsed CV\n",
    "pprint.pprint(cv_parsed_text_deepseek)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63bf0707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse JSON. Raw output:\n",
      "<think>\n",
      "Okay, let's tackle this CV parsing task. The user wants me to extract specific fields into a JSON structure based on the provided guidelines. First, I need to carefully read through the CV text and identify the relevant sections.\n",
      "\n",
      "Starting with the \"summary\" field. The guidelines mention looking for \"summary\", \"profile\", \"professional summary\", or \"about me\". Scanning the text, I see \"PROFESSIONAL SUMMARY\" followed by a paragraph. That's the summary. I'll extract that text.\n",
      "\n",
      "Next, the \"education\" section. The guidelines point to \"education\", \"academic\", \"certification\", or \"degrees\". In the CV, there's a section titled \"EDUCATION AND CERTIFICATIONS\" with \"University of London: Bachelor’s Degree in Banking and Finance\". I'll parse that into an education entry. The dates aren't mentioned here, so I'll leave start_date and end_date as empty or maybe check if there's any date info. Wait, the text says \"Bachelor’s Degree in Banking and Finance\" but no dates. The user's instruction says if a section is missing, return empty arrays or strings. Since there's no date info, maybe those fields are optional? Wait, the schema requires start_date and end_date. Hmm, but the education entry here doesn't have dates. The user's example might have dates, but in this case, the CV text doesn't mention dates for education. So I'll have to leave start_date and end_date as empty strings? Or maybe the user expects them to be optional. Wait, looking back at the schema, the education array requires each object to have start_date and end_date. But in the provided CV text, the education entry doesn't have dates. So perhaps the user expects that if dates aren't present, those fields are omitted? Wait, no, the schema says they must be present. But the user's instruction says if a section is missing, return [] or \"\". Wait, the schema requires those fields, but if the CV text doesn't have them, maybe we can't include them. Wait, the user's instruction says: \"If a section is missing, return [] or \"\".\" So for education, if there's no date, maybe we can't include it? But the example in the CV has the education entry without dates. Wait, the user's example might have that, but the schema requires start_date and end_date. So perhaps we have to leave them as empty strings? Or maybe the user expects that if dates are missing, they are omitted. But the schema requires them. Hmm, this is a bit conflicting. Let me check the example again. The user provided a sample CV text where the education entry is \"University of London: Bachelor’s Degree in Banking and Finance\" without dates. So in that case, the education entry would have start_date and end_date as empty strings? Or maybe they are omitted? The schema says they are required, so perhaps we have to include them as empty strings. So I'll set start_date and end_date as empty strings for that entry.\n",
      "\n",
      "Moving on to \"experience\". The guidelines mention \"experience\", \"work history\", etc. The CV has several entries. Let's parse each job. The first entry is \"AI Singapore – AI Apprentice | Jan 2024 - Oct 2024\". The company is AI Singapore, job_title is AI Apprentice, dates Jan 2024 to Oct 2024. Then the next entry is \"CriAT (Credit Risk Analytics Startup) – Product Analyst | Jan 2022 - Sep 2022\". Company is CriAT, job_title Product Analyst. Then \"Various Top Tier Banks & Deloitte – Private Banking Support Manager | 2016 - 2021\". Company is Various Top Tier Banks & Deloitte, job_title Private Banking Support Manager. Then the freelance entry: \"Freelance – TMT Equity Research Assistant | Jun 2018 - Aug 2019\". Company is empty, job_title is TMT Equity Research Assistant. Each of these entries has a description. The descriptions are the bullet points under each job. For example, for AI Apprentice, the description includes the points about enhancing clinician efficiency, etc. I need to extract each of these as an experience entry with job_title, company, start_date, end_date, and description.\n",
      "\n",
      "Next, \"skills\" are under \"TECHNICAL SKILLS\" section. The text lists \"Programming & ML: Python, SQL, PyTorch, Scikit-learn, FastAPI, LangChain, Deep Learning, NLP, Transformers\" and so on. I'll split these into individual skills. The user's schema requires an array of strings, so each skill item should be a separate string. For example, \"Python\", \"SQL\", etc. Also, \"LLM Ecosystem: OpenAI, OLLAMA, GROQ, LangGraph, CrewAI, Hugging Face, RAG\" should be split into individual skills. Similarly for MLOps, Cloud, Data Tools, etc. The user's example in the skills section has entries like \"Financial Modeling (Netflix forecast)\", which should be included as well. So compiling all these into an array of strings.\n",
      "\n",
      "For \"projects\", the guidelines mention \"projects\", \"project\", etc. The CV has a section titled \"PROJECTS\" with entries like \"RAG-Powered Chatbot for PDF Search\" and others. Each project has a title and a description. For example, \"RAG-Powered Chatbot for PDF Search\" has a description about designing and deploying the chatbot, using LangChain, GROQ, etc. Each project entry should be an object with project_title and description.\n",
      "\n",
      "Now, I need to make sure all the fields are correctly mapped. Let me check each section again.\n",
      "\n",
      "Summary: The text under \"PROFESSIONAL SUMMARY\" is the summary. That's the first paragraph. So the summary field is that text.\n",
      "\n",
      "Education: Only one entry, University of London with Bachelor's in Banking and Finance. No dates, so start_date and end_date are empty.\n",
      "\n",
      "Experience: Four entries. The first three are full-time roles, the fourth is freelance. Each has company, job_title, dates, and description. The descriptions include the bullet points. For example, the AI Apprentice has multiple bullet points, which should be combined into a single description string, removing any duplicates.\n",
      "\n",
      "Skills: The skills section lists multiple categories. Each item should be split into individual skills. For example, \"Python\", \"SQL\", etc. Also, the \"LLM Ecosystem\" entries are separate skills. Need to make sure all are included.\n",
      "\n",
      "Projects: The projects listed under \"PROJECTS\" have titles and descriptions. Each project's description includes the details of what was done. For example, the RAG-Powered Chatbot project has a description about designing the chatbot and the tools used.\n",
      "\n",
      "Now, I'll structure the JSON accordingly, ensuring that all fields are present and follow the schema. Also, check for any missing sections. For example, the education section has no dates, so start_date and end_date are empty. The experience entries have dates in mm/yyyy or yyyy format. The skills are split into individual strings. Projects have project_title and description.\n",
      "\n",
      "Finally, ensure that the JSON is valid, with proper syntax, no trailing commas, and all required fields present. Also, avoid any markdown formatting, just the JSON inside the code block.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"summary\": \"AI/ML Engineer with a unique blend of expertise in Python, Deep Learning, LLMs (RAG), and MLOps, coupled with a strong background in financial compliance and product analytics. I leverage deep analytical rigor and practical experience to develop and deploy data-driven solutions, particularly in regulated environments, as demonstrated by a 15% improvement in clinical decision-making efficiency through end-to-end ML pipelines.\",\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"institution\": \"University of London\",\n",
      "      \"degree\": \"Bachelor’s Degree in Banking and Finance\",\n",
      "      \"start_date\": \"\",\n",
      "      \"end_date\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"job_title\": \"AI Apprentice\",\n",
      "      \"company\": \"AI Singapore\",\n",
      "      \"start_date\": \"Jan 2024\",\n",
      "      \"end_date\": \"Oct 2024\",\n",
      "      \"description\": \"Enhanced clinician efficiency by 15% through an end-to-end ML pipeline (20 data sources, 4 engineers) that enabled prioritization of low-confidence daily predictions. Eliminated the need for clinicians to manually extract symptoms from medical reports, enhancing efficiency by collaborating on fine-tuning a biomedical BERT LLM that improved feature F1 score by 25%. Developed a customized date aggregation module using the Least Squares method to handle irregularities in time-series data, resulting in a 10% improvement in explainability boosting model F1-score. Containerized the ML pipeline with Docker and integrated FastAPI for testing, ensuring OS-agnostic deployment and robust validation across Windows, Linux, and Mac. Mentored 8 junior apprentices in Neural Networks and NLP, fostering collaboration and skill development.\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Product Analyst\",\n",
      "      \"company\": \"CriAT (Credit Risk Analytics Startup)\",\n",
      "      \"start_date\": \"Jan 2022\",\n",
      "      \"end_date\": \"Sep 2022\",\n",
      "      \"description\": \"Benchmarked internal Probability of Default (PD) models against competitors (Moody’s/S&P) standards using Power BI, delivering actionable insights to sales teams to improve competitive positioning. Engineered Python scripts to automate data quality assurance for credit risk irregular spikes alerts, enhancing daily QA efficiency by 25% and eliminating over 8 hours of manual review weekly. Started data migration analysis from Bloomberg to FactSet, validating at least 90% company coverage within product.\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Private Banking Support Manager\",\n",
      "      \"company\": \"Various Top Tier Banks & Deloitte\",\n",
      "      \"start_date\": \"2016\",\n",
      "      \"end_date\": \"2021\",\n",
      "      \"description\": \"Reduced client onboarding (KYC) turnaround from 5 days to 4 days (20%) by streamlining document verification workflows. Managed complex regulatory compliance processes (KYC, CDD, transaction monitoring, fraud prevention) across diverse market segments, ensuring adherence to stringent industry standards.\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"TMT Equity Research Assistant\",\n",
      "      \"company\": \"\",\n",
      "      \"start_date\": \"Jun 2018\",\n",
      "      \"end_date\": \"Aug 2019\",\n",
      "      \"description\": \"Built financial models and evaluated SEC reports to forecast earnings and profitability for listed and pre-IPO companies.\"\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"SQL\",\n",
      "    \"PyTorch\",\n",
      "    \"Scikit-learn\",\n",
      "    \"FastAPI\",\n",
      "    \"LangChain\",\n",
      "    \"Deep Learning\",\n",
      "    \"NLP\",\n",
      "    \"Transformers\",\n",
      "    \"OpenAI\",\n",
      "    \"OLLAMA\",\n",
      "    \"GROQ\",\n",
      "    \"LangGraph\",\n",
      "    \"CrewAI\",\n",
      "    \"Hugging Face\",\n",
      "    \"RAG\",\n",
      "    \"MLOps\",\n",
      "    \"AWS (EC2, ECR)\",\n",
      "    \"Azure (Container registry, Web App)\",\n",
      "    \"Docker\",\n",
      "    \"CI/CD (GitHub Actions)\",\n",
      "    \"MLflow\",\n",
      "    \"DagsHub\",\n",
      "    \"Power BI\",\n",
      "    \"FAISS\",\n",
      "    \"Chroma\",\n",
      "    \"Astra DB\",\n",
      "    \"Financial Modeling (Netflix forecast)\",\n",
      "    \"Financial Analysis\",\n",
      "    \"Compliance\",\n",
      "    \"AI/ML & Analytics: Associate AI Engineer (AI Singapore)\",\n",
      "    \"Business & Data Analytics (BCG)\",\n",
      "    \"Corporate Finance Institute® (CFI): Financial Modeling & Valuation Analyst\"\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"project_title\": \"RAG-Powered Chatbot for PDF Search\",\n",
      "      \"description\": \"Designed and deployed a chatbot capable of searching and answering queries from PDF documents using Retrieval-Augmented Generation (RAG) techniques. Utilized LangChain for prompt engineering, GROQ for efficient inference, and integrated with cloud storage for document retrieval.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_title\": \"Explainability Boosting in Time-Series Analysis\",\n",
      "      \"description\": \"Developed a module to enhance model interpretability in time-series forecasting by incorporating feature importance analysis and custom aggregation techniques, leading to a 10% improvement in model transparency.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_title\": \"Automated Compliance Monitoring System\",\n",
      "      \"description\": \"Engineered a system to automate regulatory compliance checks for financial institutions, reducing manual review hours by 25% and ensuring adherence to KYC and AML standards through real-time data validation.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "('<think>\\n'\n",
      " \"Okay, let's tackle this CV parsing task. The user wants me to extract \"\n",
      " 'specific fields into a JSON structure based on the provided guidelines. '\n",
      " 'First, I need to carefully read through the CV text and identify the '\n",
      " 'relevant sections.\\n'\n",
      " '\\n'\n",
      " 'Starting with the \"summary\" field. The guidelines mention looking for '\n",
      " '\"summary\", \"profile\", \"professional summary\", or \"about me\". Scanning the '\n",
      " 'text, I see \"PROFESSIONAL SUMMARY\" followed by a paragraph. That\\'s the '\n",
      " \"summary. I'll extract that text.\\n\"\n",
      " '\\n'\n",
      " 'Next, the \"education\" section. The guidelines point to \"education\", '\n",
      " '\"academic\", \"certification\", or \"degrees\". In the CV, there\\'s a section '\n",
      " 'titled \"EDUCATION AND CERTIFICATIONS\" with \"University of London: Bachelor’s '\n",
      " 'Degree in Banking and Finance\". I\\'ll parse that into an education entry. '\n",
      " \"The dates aren't mentioned here, so I'll leave start_date and end_date as \"\n",
      " \"empty or maybe check if there's any date info. Wait, the text says \"\n",
      " '\"Bachelor’s Degree in Banking and Finance\" but no dates. The user\\'s '\n",
      " 'instruction says if a section is missing, return empty arrays or strings. '\n",
      " \"Since there's no date info, maybe those fields are optional? Wait, the \"\n",
      " 'schema requires start_date and end_date. Hmm, but the education entry here '\n",
      " \"doesn't have dates. The user's example might have dates, but in this case, \"\n",
      " \"the CV text doesn't mention dates for education. So I'll have to leave \"\n",
      " 'start_date and end_date as empty strings? Or maybe the user expects them to '\n",
      " 'be optional. Wait, looking back at the schema, the education array requires '\n",
      " 'each object to have start_date and end_date. But in the provided CV text, '\n",
      " \"the education entry doesn't have dates. So perhaps the user expects that if \"\n",
      " \"dates aren't present, those fields are omitted? Wait, no, the schema says \"\n",
      " \"they must be present. But the user's instruction says if a section is \"\n",
      " 'missing, return [] or \"\". Wait, the schema requires those fields, but if the '\n",
      " \"CV text doesn't have them, maybe we can't include them. Wait, the user's \"\n",
      " 'instruction says: \"If a section is missing, return [] or \"\".\" So for '\n",
      " \"education, if there's no date, maybe we can't include it? But the example in \"\n",
      " \"the CV has the education entry without dates. Wait, the user's example might \"\n",
      " 'have that, but the schema requires start_date and end_date. So perhaps we '\n",
      " 'have to leave them as empty strings? Or maybe the user expects that if dates '\n",
      " 'are missing, they are omitted. But the schema requires them. Hmm, this is a '\n",
      " 'bit conflicting. Let me check the example again. The user provided a sample '\n",
      " 'CV text where the education entry is \"University of London: Bachelor’s '\n",
      " 'Degree in Banking and Finance\" without dates. So in that case, the education '\n",
      " 'entry would have start_date and end_date as empty strings? Or maybe they are '\n",
      " 'omitted? The schema says they are required, so perhaps we have to include '\n",
      " \"them as empty strings. So I'll set start_date and end_date as empty strings \"\n",
      " 'for that entry.\\n'\n",
      " '\\n'\n",
      " 'Moving on to \"experience\". The guidelines mention \"experience\", \"work '\n",
      " 'history\", etc. The CV has several entries. Let\\'s parse each job. The first '\n",
      " 'entry is \"AI Singapore – AI Apprentice | Jan 2024 - Oct 2024\". The company '\n",
      " 'is AI Singapore, job_title is AI Apprentice, dates Jan 2024 to Oct 2024. '\n",
      " 'Then the next entry is \"CriAT (Credit Risk Analytics Startup) – Product '\n",
      " 'Analyst | Jan 2022 - Sep 2022\". Company is CriAT, job_title Product Analyst. '\n",
      " 'Then \"Various Top Tier Banks & Deloitte – Private Banking Support Manager | '\n",
      " '2016 - 2021\". Company is Various Top Tier Banks & Deloitte, job_title '\n",
      " 'Private Banking Support Manager. Then the freelance entry: \"Freelance – TMT '\n",
      " 'Equity Research Assistant | Jun 2018 - Aug 2019\". Company is empty, '\n",
      " 'job_title is TMT Equity Research Assistant. Each of these entries has a '\n",
      " 'description. The descriptions are the bullet points under each job. For '\n",
      " 'example, for AI Apprentice, the description includes the points about '\n",
      " 'enhancing clinician efficiency, etc. I need to extract each of these as an '\n",
      " 'experience entry with job_title, company, start_date, end_date, and '\n",
      " 'description.\\n'\n",
      " '\\n'\n",
      " 'Next, \"skills\" are under \"TECHNICAL SKILLS\" section. The text lists '\n",
      " '\"Programming & ML: Python, SQL, PyTorch, Scikit-learn, FastAPI, LangChain, '\n",
      " 'Deep Learning, NLP, Transformers\" and so on. I\\'ll split these into '\n",
      " \"individual skills. The user's schema requires an array of strings, so each \"\n",
      " 'skill item should be a separate string. For example, \"Python\", \"SQL\", etc. '\n",
      " 'Also, \"LLM Ecosystem: OpenAI, OLLAMA, GROQ, LangGraph, CrewAI, Hugging Face, '\n",
      " 'RAG\" should be split into individual skills. Similarly for MLOps, Cloud, '\n",
      " \"Data Tools, etc. The user's example in the skills section has entries like \"\n",
      " '\"Financial Modeling (Netflix forecast)\", which should be included as well. '\n",
      " 'So compiling all these into an array of strings.\\n'\n",
      " '\\n'\n",
      " 'For \"projects\", the guidelines mention \"projects\", \"project\", etc. The CV '\n",
      " 'has a section titled \"PROJECTS\" with entries like \"RAG-Powered Chatbot for '\n",
      " 'PDF Search\" and others. Each project has a title and a description. For '\n",
      " 'example, \"RAG-Powered Chatbot for PDF Search\" has a description about '\n",
      " 'designing and deploying the chatbot, using LangChain, GROQ, etc. Each '\n",
      " 'project entry should be an object with project_title and description.\\n'\n",
      " '\\n'\n",
      " 'Now, I need to make sure all the fields are correctly mapped. Let me check '\n",
      " 'each section again.\\n'\n",
      " '\\n'\n",
      " 'Summary: The text under \"PROFESSIONAL SUMMARY\" is the summary. That\\'s the '\n",
      " 'first paragraph. So the summary field is that text.\\n'\n",
      " '\\n'\n",
      " \"Education: Only one entry, University of London with Bachelor's in Banking \"\n",
      " 'and Finance. No dates, so start_date and end_date are empty.\\n'\n",
      " '\\n'\n",
      " 'Experience: Four entries. The first three are full-time roles, the fourth is '\n",
      " 'freelance. Each has company, job_title, dates, and description. The '\n",
      " 'descriptions include the bullet points. For example, the AI Apprentice has '\n",
      " 'multiple bullet points, which should be combined into a single description '\n",
      " 'string, removing any duplicates.\\n'\n",
      " '\\n'\n",
      " 'Skills: The skills section lists multiple categories. Each item should be '\n",
      " 'split into individual skills. For example, \"Python\", \"SQL\", etc. Also, the '\n",
      " '\"LLM Ecosystem\" entries are separate skills. Need to make sure all are '\n",
      " 'included.\\n'\n",
      " '\\n'\n",
      " 'Projects: The projects listed under \"PROJECTS\" have titles and descriptions. '\n",
      " \"Each project's description includes the details of what was done. For \"\n",
      " 'example, the RAG-Powered Chatbot project has a description about designing '\n",
      " 'the chatbot and the tools used.\\n'\n",
      " '\\n'\n",
      " \"Now, I'll structure the JSON accordingly, ensuring that all fields are \"\n",
      " 'present and follow the schema. Also, check for any missing sections. For '\n",
      " 'example, the education section has no dates, so start_date and end_date are '\n",
      " 'empty. The experience entries have dates in mm/yyyy or yyyy format. The '\n",
      " 'skills are split into individual strings. Projects have project_title and '\n",
      " 'description.\\n'\n",
      " '\\n'\n",
      " 'Finally, ensure that the JSON is valid, with proper syntax, no trailing '\n",
      " 'commas, and all required fields present. Also, avoid any markdown '\n",
      " 'formatting, just the JSON inside the code block.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " '```json\\n'\n",
      " '{\\n'\n",
      " '  \"summary\": \"AI/ML Engineer with a unique blend of expertise in Python, '\n",
      " 'Deep Learning, LLMs (RAG), and MLOps, coupled with a strong background in '\n",
      " 'financial compliance and product analytics. I leverage deep analytical rigor '\n",
      " 'and practical experience to develop and deploy data-driven solutions, '\n",
      " 'particularly in regulated environments, as demonstrated by a 15% improvement '\n",
      " 'in clinical decision-making efficiency through end-to-end ML pipelines.\",\\n'\n",
      " '  \"education\": [\\n'\n",
      " '    {\\n'\n",
      " '      \"institution\": \"University of London\",\\n'\n",
      " '      \"degree\": \"Bachelor’s Degree in Banking and Finance\",\\n'\n",
      " '      \"start_date\": \"\",\\n'\n",
      " '      \"end_date\": \"\"\\n'\n",
      " '    }\\n'\n",
      " '  ],\\n'\n",
      " '  \"experience\": [\\n'\n",
      " '    {\\n'\n",
      " '      \"job_title\": \"AI Apprentice\",\\n'\n",
      " '      \"company\": \"AI Singapore\",\\n'\n",
      " '      \"start_date\": \"Jan 2024\",\\n'\n",
      " '      \"end_date\": \"Oct 2024\",\\n'\n",
      " '      \"description\": \"Enhanced clinician efficiency by 15% through an '\n",
      " 'end-to-end ML pipeline (20 data sources, 4 engineers) that enabled '\n",
      " 'prioritization of low-confidence daily predictions. Eliminated the need for '\n",
      " 'clinicians to manually extract symptoms from medical reports, enhancing '\n",
      " 'efficiency by collaborating on fine-tuning a biomedical BERT LLM that '\n",
      " 'improved feature F1 score by 25%. Developed a customized date aggregation '\n",
      " 'module using the Least Squares method to handle irregularities in '\n",
      " 'time-series data, resulting in a 10% improvement in explainability boosting '\n",
      " 'model F1-score. Containerized the ML pipeline with Docker and integrated '\n",
      " 'FastAPI for testing, ensuring OS-agnostic deployment and robust validation '\n",
      " 'across Windows, Linux, and Mac. Mentored 8 junior apprentices in Neural '\n",
      " 'Networks and NLP, fostering collaboration and skill development.\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"job_title\": \"Product Analyst\",\\n'\n",
      " '      \"company\": \"CriAT (Credit Risk Analytics Startup)\",\\n'\n",
      " '      \"start_date\": \"Jan 2022\",\\n'\n",
      " '      \"end_date\": \"Sep 2022\",\\n'\n",
      " '      \"description\": \"Benchmarked internal Probability of Default (PD) '\n",
      " 'models against competitors (Moody’s/S&P) standards using Power BI, '\n",
      " 'delivering actionable insights to sales teams to improve competitive '\n",
      " 'positioning. Engineered Python scripts to automate data quality assurance '\n",
      " 'for credit risk irregular spikes alerts, enhancing daily QA efficiency by '\n",
      " '25% and eliminating over 8 hours of manual review weekly. Started data '\n",
      " 'migration analysis from Bloomberg to FactSet, validating at least 90% '\n",
      " 'company coverage within product.\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"job_title\": \"Private Banking Support Manager\",\\n'\n",
      " '      \"company\": \"Various Top Tier Banks & Deloitte\",\\n'\n",
      " '      \"start_date\": \"2016\",\\n'\n",
      " '      \"end_date\": \"2021\",\\n'\n",
      " '      \"description\": \"Reduced client onboarding (KYC) turnaround from 5 days '\n",
      " 'to 4 days (20%) by streamlining document verification workflows. Managed '\n",
      " 'complex regulatory compliance processes (KYC, CDD, transaction monitoring, '\n",
      " 'fraud prevention) across diverse market segments, ensuring adherence to '\n",
      " 'stringent industry standards.\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"job_title\": \"TMT Equity Research Assistant\",\\n'\n",
      " '      \"company\": \"\",\\n'\n",
      " '      \"start_date\": \"Jun 2018\",\\n'\n",
      " '      \"end_date\": \"Aug 2019\",\\n'\n",
      " '      \"description\": \"Built financial models and evaluated SEC reports to '\n",
      " 'forecast earnings and profitability for listed and pre-IPO companies.\"\\n'\n",
      " '    }\\n'\n",
      " '  ],\\n'\n",
      " '  \"skills\": [\\n'\n",
      " '    \"Python\",\\n'\n",
      " '    \"SQL\",\\n'\n",
      " '    \"PyTorch\",\\n'\n",
      " '    \"Scikit-learn\",\\n'\n",
      " '    \"FastAPI\",\\n'\n",
      " '    \"LangChain\",\\n'\n",
      " '    \"Deep Learning\",\\n'\n",
      " '    \"NLP\",\\n'\n",
      " '    \"Transformers\",\\n'\n",
      " '    \"OpenAI\",\\n'\n",
      " '    \"OLLAMA\",\\n'\n",
      " '    \"GROQ\",\\n'\n",
      " '    \"LangGraph\",\\n'\n",
      " '    \"CrewAI\",\\n'\n",
      " '    \"Hugging Face\",\\n'\n",
      " '    \"RAG\",\\n'\n",
      " '    \"MLOps\",\\n'\n",
      " '    \"AWS (EC2, ECR)\",\\n'\n",
      " '    \"Azure (Container registry, Web App)\",\\n'\n",
      " '    \"Docker\",\\n'\n",
      " '    \"CI/CD (GitHub Actions)\",\\n'\n",
      " '    \"MLflow\",\\n'\n",
      " '    \"DagsHub\",\\n'\n",
      " '    \"Power BI\",\\n'\n",
      " '    \"FAISS\",\\n'\n",
      " '    \"Chroma\",\\n'\n",
      " '    \"Astra DB\",\\n'\n",
      " '    \"Financial Modeling (Netflix forecast)\",\\n'\n",
      " '    \"Financial Analysis\",\\n'\n",
      " '    \"Compliance\",\\n'\n",
      " '    \"AI/ML & Analytics: Associate AI Engineer (AI Singapore)\",\\n'\n",
      " '    \"Business & Data Analytics (BCG)\",\\n'\n",
      " '    \"Corporate Finance Institute® (CFI): Financial Modeling & Valuation '\n",
      " 'Analyst\"\\n'\n",
      " '  ],\\n'\n",
      " '  \"projects\": [\\n'\n",
      " '    {\\n'\n",
      " '      \"project_title\": \"RAG-Powered Chatbot for PDF Search\",\\n'\n",
      " '      \"description\": \"Designed and deployed a chatbot capable of searching '\n",
      " 'and answering queries from PDF documents using Retrieval-Augmented '\n",
      " 'Generation (RAG) techniques. Utilized LangChain for prompt engineering, GROQ '\n",
      " 'for efficient inference, and integrated with cloud storage for document '\n",
      " 'retrieval.\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"project_title\": \"Explainability Boosting in Time-Series Analysis\",\\n'\n",
      " '      \"description\": \"Developed a module to enhance model interpretability '\n",
      " 'in time-series forecasting by incorporating feature importance analysis and '\n",
      " 'custom aggregation techniques, leading to a 10% improvement in model '\n",
      " 'transparency.\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"project_title\": \"Automated Compliance Monitoring System\",\\n'\n",
      " '      \"description\": \"Engineered a system to automate regulatory compliance '\n",
      " 'checks for financial institutions, reducing manual review hours by 25% and '\n",
      " 'ensuring adherence to KYC and AML standards through real-time data '\n",
      " 'validation.\"\\n'\n",
      " '    }\\n'\n",
      " '  ]\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "cv_parsed_text_qwen= llm_qwen.invoke(cv_prompt)\n",
    "\n",
    "# Inspect the parsed CV\n",
    "pprint.pprint(cv_parsed_text_qwen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0811cdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse JSON. Raw output:\n",
      "<think>\n",
      "Okay, let's tackle this CV parsing task. The user wants me to extract specific fields into a JSON structure based on the provided guidelines. First, I need to carefully read through the CV text and identify the relevant sections.\n",
      "\n",
      "Starting with the \"summary\" field. The guidelines mention looking for \"summary\", \"profile\", \"professional summary\", or \"about me\". Scanning the text, I see \"PROFESSIONAL SUMMARY\" followed by a paragraph. That's the summary. I'll extract that text.\n",
      "\n",
      "Next, the \"education\" section. The guidelines point to \"education\", \"academic\", \"certification\", or \"degrees\". In the CV, there's a section titled \"EDUCATION AND CERTIFICATIONS\" with \"University of London: Bachelor’s Degree in Banking and Finance\". I'll parse that into an education entry. The dates aren't mentioned here, so I'll leave start_date and end_date as empty or maybe check if there's any date info. Wait, the text says \"Bachelor’s Degree in Banking and Finance\" but no dates. The user's instruction says if a section is missing, return empty arrays or strings. Since there's no date info, maybe those fields are optional? Wait, the schema requires start_date and end_date. Hmm, but the education entry here doesn't have dates. The user's example might have dates, but in this case, the CV text doesn't mention dates for education. So I'll have to leave start_date and end_date as empty strings? Or maybe the user expects them to be optional. Wait, looking back at the schema, the education array requires each object to have start_date and end_date. But in the provided CV text, the education entry doesn't have dates. So perhaps the user expects that if dates aren't present, those fields are omitted? Wait, no, the schema says they must be present. But the user's instruction says if a section is missing, return [] or \"\". Wait, the schema requires those fields, but if the CV text doesn't have them, maybe we can't include them. Wait, the user's instruction says: \"If a section is missing, return [] or \"\".\" So for education, if there's no date, maybe we can't include it? But the example in the CV has the education entry without dates. Wait, the user's example might have that, but the schema requires start_date and end_date. So perhaps we have to leave them as empty strings? Or maybe the user expects that if dates are missing, they are omitted. But the schema requires them. Hmm, this is a bit conflicting. Let me check the example again. The user provided a sample CV text where the education entry is \"University of London: Bachelor’s Degree in Banking and Finance\" without dates. So in that case, the education entry would have start_date and end_date as empty strings? Or maybe they are omitted? The schema says they are required, so perhaps we have to include them as empty strings. So I'll set start_date and end_date as empty strings for that entry.\n",
      "\n",
      "Moving on to \"experience\". The guidelines mention \"experience\", \"work history\", etc. The CV has several entries. Let's parse each job. The first entry is \"AI Singapore – AI Apprentice | Jan 2024 - Oct 2024\". The company is AI Singapore, job_title is AI Apprentice, dates Jan 2024 to Oct 2024. Then the next entry is \"CriAT (Credit Risk Analytics Startup) – Product Analyst | Jan 2022 - Sep 2022\". Company is CriAT, job_title Product Analyst. Then \"Various Top Tier Banks & Deloitte – Private Banking Support Manager | 2016 - 2021\". Company is Various Top Tier Banks & Deloitte, job_title Private Banking Support Manager. Then the freelance entry: \"Freelance – TMT Equity Research Assistant | Jun 2018 - Aug 2019\". Company is empty, job_title is TMT Equity Research Assistant. Each of these entries has a description. The descriptions are the bullet points under each job. For example, for AI Apprentice, the description includes the points about enhancing clinician efficiency, etc. I need to extract each of these as an experience entry with job_title, company, start_date, end_date, and description.\n",
      "\n",
      "Next, \"skills\" are under \"TECHNICAL SKILLS\" section. The text lists \"Programming & ML: Python, SQL, PyTorch, Scikit-learn, FastAPI, LangChain, Deep Learning, NLP, Transformers\" and so on. I'll split these into individual skills. The user's schema requires an array of strings, so each skill item should be a separate string. For example, \"Python\", \"SQL\", etc. Also, \"LLM Ecosystem: OpenAI, OLLAMA, GROQ, LangGraph, CrewAI, Hugging Face, RAG\" should be split into individual skills. Similarly for MLOps, Cloud, Data Tools, etc. The user's example in the skills section has entries like \"Financial Modeling (Netflix forecast)\", which should be included as well. So compiling all these into an array of strings.\n",
      "\n",
      "For \"projects\", the guidelines mention \"projects\", \"project\", etc. The CV has a section titled \"PROJECTS\" with entries like \"RAG-Powered Chatbot for PDF Search\" and others. Each project has a title and a description. For example, \"RAG-Powered Chatbot for PDF Search\" has a description about designing and deploying the chatbot, using LangChain, GROQ, etc. Each project entry should be an object with project_title and description.\n",
      "\n",
      "Now, I need to make sure all the fields are correctly mapped. Let me check each section again.\n",
      "\n",
      "Summary: The text under \"PROFESSIONAL SUMMARY\" is the summary. That's the first paragraph. So the summary field is that text.\n",
      "\n",
      "Education: Only one entry, University of London with Bachelor's in Banking and Finance. No dates, so start_date and end_date are empty.\n",
      "\n",
      "Experience: Four entries. The first three are full-time roles, the fourth is freelance. Each has company, job_title, dates, and description. The descriptions include the bullet points. For example, the AI Apprentice has multiple bullet points, which should be combined into a single description string, removing any duplicates.\n",
      "\n",
      "Skills: The skills section lists multiple categories. Each item should be split into individual skills. For example, \"Python\", \"SQL\", etc. Also, the \"LLM Ecosystem\" entries are separate skills. Need to make sure all are included.\n",
      "\n",
      "Projects: The projects listed under \"PROJECTS\" have titles and descriptions. Each project's description includes the details of what was done. For example, the RAG-Powered Chatbot project has a description about designing the chatbot and the tools used.\n",
      "\n",
      "Now, I'll structure the JSON accordingly, ensuring that all fields are present and follow the schema. Also, check for any missing sections. For example, the education section has no dates, so start_date and end_date are empty. The experience entries have dates in mm/yyyy or yyyy format. The skills are split into individual strings. Projects have project_title and description.\n",
      "\n",
      "Finally, ensure that the JSON is valid, with proper syntax, no trailing commas, and all required fields present. Also, avoid any markdown formatting, just the JSON inside the code block.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"summary\": \"AI/ML Engineer with a unique blend of expertise in Python, Deep Learning, LLMs (RAG), and MLOps, coupled with a strong background in financial compliance and product analytics. I leverage deep analytical rigor and practical experience to develop and deploy data-driven solutions, particularly in regulated environments, as demonstrated by a 15% improvement in clinical decision-making efficiency through end-to-end ML pipelines.\",\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"institution\": \"University of London\",\n",
      "      \"degree\": \"Bachelor’s Degree in Banking and Finance\",\n",
      "      \"start_date\": \"\",\n",
      "      \"end_date\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"job_title\": \"AI Apprentice\",\n",
      "      \"company\": \"AI Singapore\",\n",
      "      \"start_date\": \"Jan 2024\",\n",
      "      \"end_date\": \"Oct 2024\",\n",
      "      \"description\": \"Enhanced clinician efficiency by 15% through an end-to-end ML pipeline (20 data sources, 4 engineers) that enabled prioritization of low-confidence daily predictions. Eliminated the need for clinicians to manually extract symptoms from medical reports, enhancing efficiency by collaborating on fine-tuning a biomedical BERT LLM that improved feature F1 score by 25%. Developed a customized date aggregation module using the Least Squares method to handle irregularities in time-series data, resulting in a 10% improvement in explainability boosting model F1-score. Containerized the ML pipeline with Docker and integrated FastAPI for testing, ensuring OS-agnostic deployment and robust validation across Windows, Linux, and Mac. Mentored 8 junior apprentices in Neural Networks and NLP, fostering collaboration and skill development.\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Product Analyst\",\n",
      "      \"company\": \"CriAT (Credit Risk Analytics Startup)\",\n",
      "      \"start_date\": \"Jan 2022\",\n",
      "      \"end_date\": \"Sep 2022\",\n",
      "      \"description\": \"Benchmarked internal Probability of Default (PD) models against competitors (Moody’s/S&P) standards using Power BI, delivering actionable insights to sales teams to improve competitive positioning. Engineered Python scripts to automate data quality assurance for credit risk irregular spikes alerts, enhancing daily QA efficiency by 25% and eliminating over 8 hours of manual review weekly. Started data migration analysis from Bloomberg to FactSet, validating at least 90% company coverage within product.\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Private Banking Support Manager\",\n",
      "      \"company\": \"Various Top Tier Banks & Deloitte\",\n",
      "      \"start_date\": \"2016\",\n",
      "      \"end_date\": \"2021\",\n",
      "      \"description\": \"Reduced client onboarding (KYC) turnaround from 5 days to 4 days (20%) by streamlining document verification workflows. Managed complex regulatory compliance processes (KYC, CDD, transaction monitoring, fraud prevention) across diverse market segments, ensuring adherence to stringent industry standards.\"\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"TMT Equity Research Assistant\",\n",
      "      \"company\": \"\",\n",
      "      \"start_date\": \"Jun 2018\",\n",
      "      \"end_date\": \"Aug 2019\",\n",
      "      \"description\": \"Built financial models and evaluated SEC reports to forecast earnings and profitability for listed and pre-IPO companies.\"\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"SQL\",\n",
      "    \"PyTorch\",\n",
      "    \"Scikit-learn\",\n",
      "    \"FastAPI\",\n",
      "    \"LangChain\",\n",
      "    \"Deep Learning\",\n",
      "    \"NLP\",\n",
      "    \"Transformers\",\n",
      "    \"OpenAI\",\n",
      "    \"OLLAMA\",\n",
      "    \"GROQ\",\n",
      "    \"LangGraph\",\n",
      "    \"CrewAI\",\n",
      "    \"Hugging Face\",\n",
      "    \"RAG\",\n",
      "    \"MLOps\",\n",
      "    \"AWS (EC2, ECR)\",\n",
      "    \"Azure (Container registry, Web App)\",\n",
      "    \"Docker\",\n",
      "    \"CI/CD (GitHub Actions)\",\n",
      "    \"MLflow\",\n",
      "    \"DagsHub\",\n",
      "    \"Power BI\",\n",
      "    \"FAISS\",\n",
      "    \"Chroma\",\n",
      "    \"Astra DB\",\n",
      "    \"Financial Modeling (Netflix forecast)\",\n",
      "    \"Financial Analysis\",\n",
      "    \"Compliance\",\n",
      "    \"AI/ML & Analytics: Associate AI Engineer (AI Singapore)\",\n",
      "    \"Business & Data Analytics (BCG)\",\n",
      "    \"Corporate Finance Institute® (CFI): Financial Modeling & Valuation Analyst\"\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"project_title\": \"RAG-Powered Chatbot for PDF Search\",\n",
      "      \"description\": \"Designed and deployed a chatbot capable of searching and answering queries from PDF documents using Retrieval-Augmented Generation (RAG) techniques. Utilized LangChain for prompt engineering, GROQ for efficient inference, and integrated with cloud storage for document retrieval.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_title\": \"Explainability Boosting in Time-Series Analysis\",\n",
      "      \"description\": \"Developed a module to enhance model interpretability in time-series forecasting by incorporating feature importance analysis and custom aggregation techniques, leading to a 10% improvement in model transparency.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_title\": \"Automated Compliance Monitoring System\",\n",
      "      \"description\": \"Engineered a system to automate regulatory compliance checks for financial institutions, reducing manual review hours by 25% and ensuring adherence to KYC and AML standards through real-time data validation.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cv_json = json.loads(cv_parsed_text_qwen)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to parse JSON. Raw output:\")\n",
    "    print(cv_parsed_text_qwen)  # print the raw text, not cv_json\n",
    "    cv_json = None\n",
    "\n",
    "# Inspect the parsed CV (if parsing succeeded, cv_json is a dict)\n",
    "if cv_json:\n",
    "    pprint.pprint(cv_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af87efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_into_json(raw_input):\n",
    "\n",
    "    raw_output = raw_input  \n",
    "\n",
    "    # 1. Remove <think> blocks if they exist\n",
    "    cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", raw_output, flags=re.DOTALL)\n",
    "\n",
    "    # 2. Extract the JSON inside ```json ... ```\n",
    "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", cleaned_text, flags=re.DOTALL)\n",
    "    if match:\n",
    "        json_text = match.group(1)\n",
    "    else:\n",
    "        # fallback: extract from first { to last }\n",
    "        start = cleaned_text.find(\"{\")\n",
    "        end = cleaned_text.rfind(\"}\")\n",
    "        json_text = cleaned_text[start:end+1]\n",
    "\n",
    "    # 3. Load JSON\n",
    "    try:\n",
    "        cv_json = json.loads(json_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Failed to parse JSON:\", e)\n",
    "        print(\"Raw JSON text:\")\n",
    "        print(json_text)\n",
    "        cv_json = None\n",
    "        \n",
    "    return pprint(cv_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0109dfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'education': [{'degree': 'Bachelor’s Degree in Banking and Finance',\n",
      "                'end_date': '',\n",
      "                'institution': 'University of London',\n",
      "                'start_date': ''}],\n",
      " 'experience': [{'company': 'AI Singapore',\n",
      "                 'description': 'Enhanced clinician efficiency by 15% through '\n",
      "                                'an end-to-end ML pipeline (20 data sources, 4 '\n",
      "                                'engineers) that enabled prioritization of '\n",
      "                                'low-confidence daily predictions. Eliminated '\n",
      "                                'the need for clinicians to manually extract '\n",
      "                                'symptoms from medical reports, enhancing '\n",
      "                                'efficiency by collaborating on fine-tuning a '\n",
      "                                'biomedical BERT LLM that improved feature F1 '\n",
      "                                'score by 25%. Developed a customized date '\n",
      "                                'aggregation module using the Least Squares '\n",
      "                                'method to handle irregularities in '\n",
      "                                'time-series data, resulting in a 10% '\n",
      "                                'improvement in explainability boosting model '\n",
      "                                'F1-score. Containerized the ML pipeline with '\n",
      "                                'Docker and integrated FastAPI for testing, '\n",
      "                                'ensuring OS-agnostic deployment and robust '\n",
      "                                'validation across Windows, Linux, and Mac. '\n",
      "                                'Mentored 8 junior apprentices in Neural '\n",
      "                                'Networks and NLP, fostering collaboration and '\n",
      "                                'skill development.',\n",
      "                 'end_date': 'Oct 2024',\n",
      "                 'job_title': 'AI Apprentice',\n",
      "                 'start_date': 'Jan 2024'},\n",
      "                {'company': 'CriAT (Credit Risk Analytics Startup)',\n",
      "                 'description': 'Benchmarked internal Probability of Default '\n",
      "                                '(PD) models against competitors (Moody’s/S&P) '\n",
      "                                'standards using Power BI, delivering '\n",
      "                                'actionable insights to sales teams to improve '\n",
      "                                'competitive positioning. Engineered Python '\n",
      "                                'scripts to automate data quality assurance '\n",
      "                                'for credit risk irregular spikes alerts, '\n",
      "                                'enhancing daily QA efficiency by 25% and '\n",
      "                                'eliminating over 8 hours of manual review '\n",
      "                                'weekly. Started data migration analysis from '\n",
      "                                'Bloomberg to FactSet, validating at least 90% '\n",
      "                                'company coverage within product.',\n",
      "                 'end_date': 'Sep 2022',\n",
      "                 'job_title': 'Product Analyst',\n",
      "                 'start_date': 'Jan 2022'},\n",
      "                {'company': 'Various Top Tier Banks & Deloitte',\n",
      "                 'description': 'Reduced client onboarding (KYC) turnaround '\n",
      "                                'from 5 days to 4 days (20%) by streamlining '\n",
      "                                'document verification workflows. Managed '\n",
      "                                'complex regulatory compliance processes (KYC, '\n",
      "                                'CDD, transaction monitoring, fraud '\n",
      "                                'prevention) across diverse market segments, '\n",
      "                                'ensuring adherence to stringent industry '\n",
      "                                'standards.',\n",
      "                 'end_date': '2021',\n",
      "                 'job_title': 'Private Banking Support Manager',\n",
      "                 'start_date': '2016'},\n",
      "                {'company': '',\n",
      "                 'description': 'Built financial models and evaluated SEC '\n",
      "                                'reports to forecast earnings and '\n",
      "                                'profitability for listed and pre-IPO '\n",
      "                                'companies.',\n",
      "                 'end_date': 'Aug 2019',\n",
      "                 'job_title': 'TMT Equity Research Assistant',\n",
      "                 'start_date': 'Jun 2018'}],\n",
      " 'projects': [{'description': 'Designed and deployed a chatbot capable of '\n",
      "                              'searching and answering queries from PDF '\n",
      "                              'documents using Retrieval-Augmented Generation '\n",
      "                              '(RAG) techniques. Utilized LangChain for prompt '\n",
      "                              'engineering, GROQ for efficient inference, and '\n",
      "                              'integrated with cloud storage for document '\n",
      "                              'retrieval.',\n",
      "               'project_title': 'RAG-Powered Chatbot for PDF Search'},\n",
      "              {'description': 'Developed a module to enhance model '\n",
      "                              'interpretability in time-series forecasting by '\n",
      "                              'incorporating feature importance analysis and '\n",
      "                              'custom aggregation techniques, leading to a 10% '\n",
      "                              'improvement in model transparency.',\n",
      "               'project_title': 'Explainability Boosting in Time-Series '\n",
      "                                'Analysis'},\n",
      "              {'description': 'Engineered a system to automate regulatory '\n",
      "                              'compliance checks for financial institutions, '\n",
      "                              'reducing manual review hours by 25% and '\n",
      "                              'ensuring adherence to KYC and AML standards '\n",
      "                              'through real-time data validation.',\n",
      "               'project_title': 'Automated Compliance Monitoring System'}],\n",
      " 'skills': ['Python',\n",
      "            'SQL',\n",
      "            'PyTorch',\n",
      "            'Scikit-learn',\n",
      "            'FastAPI',\n",
      "            'LangChain',\n",
      "            'Deep Learning',\n",
      "            'NLP',\n",
      "            'Transformers',\n",
      "            'OpenAI',\n",
      "            'OLLAMA',\n",
      "            'GROQ',\n",
      "            'LangGraph',\n",
      "            'CrewAI',\n",
      "            'Hugging Face',\n",
      "            'RAG',\n",
      "            'MLOps',\n",
      "            'AWS (EC2, ECR)',\n",
      "            'Azure (Container registry, Web App)',\n",
      "            'Docker',\n",
      "            'CI/CD (GitHub Actions)',\n",
      "            'MLflow',\n",
      "            'DagsHub',\n",
      "            'Power BI',\n",
      "            'FAISS',\n",
      "            'Chroma',\n",
      "            'Astra DB',\n",
      "            'Financial Modeling (Netflix forecast)',\n",
      "            'Financial Analysis',\n",
      "            'Compliance',\n",
      "            'AI/ML & Analytics: Associate AI Engineer (AI Singapore)',\n",
      "            'Business & Data Analytics (BCG)',\n",
      "            'Corporate Finance Institute® (CFI): Financial Modeling & '\n",
      "            'Valuation Analyst'],\n",
      " 'summary': 'AI/ML Engineer with a unique blend of expertise in Python, Deep '\n",
      "            'Learning, LLMs (RAG), and MLOps, coupled with a strong background '\n",
      "            'in financial compliance and product analytics. I leverage deep '\n",
      "            'analytical rigor and practical experience to develop and deploy '\n",
      "            'data-driven solutions, particularly in regulated environments, as '\n",
      "            'demonstrated by a 15% improvement in clinical decision-making '\n",
      "            'efficiency through end-to-end ML pipelines.'}\n"
     ]
    }
   ],
   "source": [
    "clean_text_into_json(cv_parsed_text_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5def8d6",
   "metadata": {},
   "source": [
    "### Challenges faced\n",
    "- Took alot of time crafting the right prompts for the LLMs to ingest the CV and output the approriate fields as JSON\n",
    "- Tried several open source models, so far qwen seems to be the best based on json outputs\n",
    "- While deepseek performs well, it always the output always shows its reasoning behind the scene, have to create a customised regex to remove the reasoning logic.\n",
    "\n",
    "### Moving on to crafting a prompt for JD(we can start with linkedIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_linkedin_text= \"\"\"About the job\n",
    "About us \n",
    "\n",
    "RE-LIVE is a next-generation real estate insights platform focused on streamlining and modernizing the valuation process for multiple commercial and residential property types. Our mission is to enable access to property intelligence through clean UI, data integrations, and dynamic reporting, with a strong emphasis on usability and accuracy. This role will contribute directly to transforming how real estate is evaluated and reported.\n",
    "\n",
    "\n",
    "Your Role\n",
    "\n",
    "As our Machine Learning Engineer, you'll spearhead the development of predictive models for real estate pricing, incorporating a rich variety of structured and unstructured data sources. You’ll work across the full ML lifecycle—from data acquisition and preprocessing to modeling, evaluation, and front-end visualization.\n",
    "\n",
    "\n",
    "Key Responsibilities\n",
    "\n",
    "Develop and deploy machine learning models to predict real estate prices and investment potential.\n",
    "Identify and transform relevant signals from heterogeneous datasets to support accurate property value predictions.\n",
    "Conduct NLP-based analysis of textual data (e.g. user reviews, market reports, real estate articles) to enrich model inputs.\n",
    "Collect, clean, merge, and manage large and diverse datasets from APIs, web scraping, public sources, and commercial databases.\n",
    "Design and implement interactive dashboards to visualize trends, model predictions, and insights in a user-friendly manner.\n",
    "Collaborate with valuation experts and product designers to integrate insights into our platform.\n",
    "\n",
    "\n",
    "Qualifications\n",
    "\n",
    "Bachelor's or Master's degree in Computer Science, Data Science, or related field.\n",
    "Strong academic background or demonstrable track record of high-impact, self-driven work in data science or machine learning.\n",
    "Strong proficiency in Python and data science libraries like Pandas, Scikit-learn, NumPy.\n",
    "Experience with deep learning frameworks (e.g. PyTorch, TensorFlow) for regression and NLP tasks.\n",
    "Hands-on experience building interactive dashboards using Dash, Plotly, or Streamlit.\n",
    "Familiarity with geospatial data and tools like GeoPandas, Shapely, or Kepler.gl is a plus.\n",
    "Bonus points for knowledge of PostgreSQL/PostGIS, Elasticsearch, or LLMs for contextual insights.\n",
    "\n",
    "\n",
    "What We Offer\n",
    "\n",
    "Opportunity to build and scale a product that will redefine real estate investing in Asia and beyond.\n",
    "Flexible working hours.\n",
    "Collaborative, innovation-driven environment with direct access to decision-makers.\n",
    "Competitive compensation and performance incentives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adacb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_prompt = f\"\"\"\n",
    "You are a Job Description (JD) parsing assistant. \n",
    "\n",
    "Extract the following fields from this JD text and return ONLY valid JSON inside a fenced code block (```json ... ```). \n",
    "Do not output anything else.\n",
    "\n",
    "Schema:\n",
    "```json\n",
    "{{\n",
    "  \"job_title\": \"As per below guidelines\",\n",
    "  \"department\": \"As per below guidelines (leave empty if not specified)\",\n",
    "  \"location\": \"As per below guidelines (leave empty if not specified)\",\n",
    "  \"employment_type\": \"As per below guidelines (Full-time, Part-time, Contract, etc.)\",\n",
    "  \"experience_required\": \"As per below guidelines (years of experience or descriptive requirements)\",\n",
    "  \"responsibilities\": [\"As per below guidelines\"],\n",
    "  \"skills_required\": [\"As per below guidelines (programming languages, tools, frameworks, technologies)\"],\n",
    "  \"qualifications\": [\"As per below guidelines (required degrees, certifications)\"],\n",
    "  \"preferred_qualifications\": [\"As per below guidelines (optional or bonus skills)\"],\n",
    "  \"other_notes\": \"As per below guidelines (benefits, company culture, perks, etc.)\"\n",
    "}}\n",
    "\n",
    "\n",
    "Use the following guidelines:\n",
    "1. job_title: Extract the main role being hired for, e.g., \"Machine Learning Engineer\".\n",
    "2. department: Include the team or department if explicitly mentioned, otherwise leave as empty string \"\".\n",
    "3. location: Extract physical or remote location if specified, otherwise leave empty.\n",
    "4. employment_type: Identify if the role is Full-time, Part-time, Contract, Internship, etc.\n",
    "5. experience_required: Include years of experience or descriptive requirements (e.g., \"2+ years of experience in data science or machine learning\").\n",
    "6. responsibilities: Extract all key tasks, duties, or responsibilities mentioned under sections like \"Responsibilities\", \"Key Responsibilities\", \"Role Overview\". Each responsibility should be a separate string in the array.\n",
    "7. skills_required: Extract all technical skills, programming languages, tools, frameworks, or technologies explicitly mentioned in the JD. Each skill should be a separate string.\n",
    "8. qualifications: Include required degrees, certifications, or educational qualifications. Each should be a separate string.\n",
    "9. preferred_qualifications: Include optional or bonus qualifications or skills. Each should be a separate string.\n",
    "10. other_notes: Include any additional information like benefits, company culture, perks, flexible working arrangements, or miscellaneous notes not captured in other fields.\n",
    "\n",
    "Return fully populated JSON. Do not use ellipses .... If a section is missing, return [] for arrays or \"\" for string fields.\n",
    "\n",
    "Here is the JD text:\n",
    "\n",
    "\"{sample_linkedin_text}\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dca03a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_parsed_text_qwen= llm_qwen.invoke(jd_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a203c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'department': '',\n",
      " 'employment_type': '',\n",
      " 'experience_required': 'Strong academic background or demonstrable track '\n",
      "                        'record of high-impact, self-driven work in data '\n",
      "                        'science or machine learning',\n",
      " 'job_title': 'Machine Learning Engineer',\n",
      " 'location': '',\n",
      " 'other_notes': 'Opportunity to build and scale a product that will redefine '\n",
      "                'real estate investing in Asia and beyond. Flexible working '\n",
      "                'hours. Collaborative, innovation-driven environment with '\n",
      "                'direct access to decision-makers. Competitive compensation '\n",
      "                'and performance incentives.',\n",
      " 'preferred_qualifications': ['Familiarity with geospatial data and tools like '\n",
      "                              'GeoPandas, Shapely, or Kepler.gl',\n",
      "                              'Knowledge of PostgreSQL/PostGIS, Elasticsearch, '\n",
      "                              'or LLMs for contextual insights'],\n",
      " 'qualifications': [\"Bachelor's or Master's degree in Computer Science, Data \"\n",
      "                    'Science, or related field'],\n",
      " 'responsibilities': ['Develop and deploy machine learning models to predict '\n",
      "                      'real estate prices and investment potential.',\n",
      "                      'Identify and transform relevant signals from '\n",
      "                      'heterogeneous datasets to support accurate property '\n",
      "                      'value predictions.',\n",
      "                      'Conduct NLP-based analysis of textual data (e.g. user '\n",
      "                      'reviews, market reports, real estate articles) to '\n",
      "                      'enrich model inputs.',\n",
      "                      'Collect, clean, merge, and manage large and diverse '\n",
      "                      'datasets from APIs, web scraping, public sources, and '\n",
      "                      'commercial databases.',\n",
      "                      'Design and implement interactive dashboards to '\n",
      "                      'visualize trends, model predictions, and insights in a '\n",
      "                      'user-friendly manner.',\n",
      "                      'Collaborate with valuation experts and product '\n",
      "                      'designers to integrate insights into our platform.'],\n",
      " 'skills_required': ['Python',\n",
      "                     'Pandas',\n",
      "                     'Scikit-learn',\n",
      "                     'NumPy',\n",
      "                     'PyTorch',\n",
      "                     'TensorFlow',\n",
      "                     'Dash',\n",
      "                     'Plotly',\n",
      "                     'Streamlit',\n",
      "                     'GeoPandas',\n",
      "                     'Shapely',\n",
      "                     'Kepler.gl',\n",
      "                     'PostgreSQL/PostGIS',\n",
      "                     'Elasticsearch',\n",
      "                     'LLMs for contextual insights']}\n"
     ]
    }
   ],
   "source": [
    "clean_text_into_json(jd_parsed_text_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec333f",
   "metadata": {},
   "source": [
    "#### Summary of JD and CV Parsing using LLMS\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-agent (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
